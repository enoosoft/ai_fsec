{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enoosoft/ai_fsec/blob/master/colab/Day_2_02_logistic_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o3l9XEWZC34"
      },
      "source": [
        "# Lab 5: Logistic Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPnvFIVNZC37"
      },
      "source": [
        "## Reminder: Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UX54HCjZC38"
      },
      "source": [
        "### Hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n-JVce4ZC38"
      },
      "source": [
        "$$ H(X) = \\frac{1}{1+e^{-W^T X}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt_ESSDdZC39"
      },
      "source": [
        "### Cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKxKlty9ZC3-"
      },
      "source": [
        "$$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9bay9QeZC3_"
      },
      "source": [
        " - If $y \\simeq H(x)$, cost is near 0.\n",
        " - If $y \\neq H(x)$, cost is high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKvXt1IKZC4A"
      },
      "source": [
        "### Weight Update via Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cBu1IoJZC4B"
      },
      "source": [
        "$$ W := W - \\alpha \\frac{\\partial}{\\partial W} cost(W) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msjfxs40ZC4C"
      },
      "source": [
        " - $\\alpha$: Learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPqtsurUZC4C"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "Gfof8h7vZC4D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxnRaiRMZC4I",
        "outputId": "5b763311-54fa-4d7b-a11e-6d2a144bcfc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f27c6b95910>"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "# For reproducibility\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXWLQJTZZC4M"
      },
      "source": [
        "## Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "qM8zauOFZC4N"
      },
      "outputs": [],
      "source": [
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]] # 2x2 (국어공부시간, 수학공부시간)\n",
        "y_data = [[0], [0], [0], [1], [1], [1]] # 합격=1/불합격=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXnegPJSZC4Q"
      },
      "source": [
        "Consider the following classification problem: given the number of hours each student spent watching the lecture and working in the code lab, predict whether the student passed or failed a course. For example, the first (index 0) student watched the lecture for 1 hour and spent 2 hours in the lab session ([1, 2]), and ended up failing the course ([0])."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "wXSz-wnWZC4R"
      },
      "outputs": [],
      "source": [
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VB3nj6pZC4U"
      },
      "source": [
        "As always, we need these data to be in `torch.Tensor` format, so we convert them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4UEDSFCZC4V",
        "outputId": "0460f159-fd66-4650-a760-48dd486d059b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 2])\n",
            "torch.Size([6, 1])\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQTCMvLnZC4X"
      },
      "source": [
        "## Computing the Hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTVPwmsLZC4Y"
      },
      "source": [
        "$$ H(X) = \\frac{1}{1+e^{-W^T X}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfbhP0MFZC4Y"
      },
      "source": [
        "PyTorch has a `torch.exp()` function that resembles the exponential function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO476G3zZC4Y",
        "outputId": "aaa769d1-4591-4975-c436-ffe53ec9a213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e^1 equals:  tensor([2.7183])\n"
          ]
        }
      ],
      "source": [
        "print('e^1 equals: ', torch.exp(torch.FloatTensor([1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3zRIcUaZC4a"
      },
      "source": [
        "We can use it to compute the hypothesis function conveniently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "v2GIFJVNZC4b"
      },
      "outputs": [],
      "source": [
        "W = torch.zeros((2, 1), requires_grad=True)  # 학습의 대상이다 = True\n",
        "b = torch.zeros(1, requires_grad=True)       # 학습의 대상이다 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "qG_zwf3VZC4d"
      },
      "outputs": [],
      "source": [
        "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSqt830nZC4g",
        "outputId": "faa110af-16f3-4880-efa4-04ed3184d4b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000]], grad_fn=<MulBackward0>)\n",
            "torch.Size([6, 1])\n"
          ]
        }
      ],
      "source": [
        "print(hypothesis)\n",
        "print(hypothesis.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSj4A-RbZC4i"
      },
      "source": [
        "Or, we could use `torch.sigmoid()` function! This resembles the sigmoid function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_fsm8xkZC4j",
        "outputId": "3f9090c8-ddca-45a8-f5a1-757f6535fe8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/(1+e^{-1}) equals:  tensor([0.7311])\n"
          ]
        }
      ],
      "source": [
        "print('1/(1+e^{-1}) equals: ', torch.sigmoid(torch.FloatTensor([1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0J9yOL4ZC4l"
      },
      "source": [
        "Now, the code for hypothesis function is cleaner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "kiFi8sH8ZC4l"
      },
      "outputs": [],
      "source": [
        "hypothesis = torch.sigmoid(x_train.matmul(W) + b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1quueDJLZC4o",
        "outputId": "7d6cf941-f2bf-4ce2-827a-8a2a7ce7dffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000]], grad_fn=<SigmoidBackward0>)\n",
            "torch.Size([6, 1])\n"
          ]
        }
      ],
      "source": [
        "print(hypothesis)\n",
        "print(hypothesis.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiO0wnrqZC4q"
      },
      "source": [
        "## Computing the Cost Function (Low-level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uzOm9AkZC4q"
      },
      "source": [
        "$$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVkNZuoVZC4r"
      },
      "source": [
        "We want to measure the difference between `hypothesis` and `y_train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQa90HhTZC4r",
        "outputId": "74eae857-3d4e-4135-b1fb-ad5897d602f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000],\n",
            "        [0.5000]], grad_fn=<SigmoidBackward0>)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ],
      "source": [
        "print(hypothesis)\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ0StNy0ZC4u"
      },
      "source": [
        "For one element, the loss can be computed as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-s-N0N6ZC4u",
        "outputId": "15e73ab8-8f87-4ebe-d579-7a0403529503"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6931], grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "source": [
        "-(y_train[0] * torch.log(hypothesis[0]) + \n",
        "  (1 - y_train[0]) * torch.log(1 - hypothesis[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxYpTgbWZC4w"
      },
      "source": [
        "To compute the losses for the entire batch, we can simply input the entire vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu3NdNnRZC4x",
        "outputId": "3ea7f17f-32d9-4d9a-e5b6-e2e7d692a367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931],\n",
            "        [0.6931]], grad_fn=<NegBackward0>)\n"
          ]
        }
      ],
      "source": [
        "losses = -(y_train * torch.log(hypothesis) + \n",
        "           (1 - y_train) * torch.log(1 - hypothesis))\n",
        "print(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kewkA4yZC4z"
      },
      "source": [
        "Then, we just `.mean()` to take the mean of these individual losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnKxi8ijZC4z",
        "outputId": "983843f5-54c1-4dde-e556-fb5d957bcaaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "cost = losses.mean()\n",
        "print(cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H1-1rXqZC43"
      },
      "source": [
        "## Computing the Cost Function with `F.binary_cross_entropy`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJM4sPlcZC43"
      },
      "source": [
        "In reality, binary classification is used so often that PyTorch has a simple function called `F.binary_cross_entropy` implemented to lighten the burden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv9XleyMZC43",
        "outputId": "b0f4a231-8ace-4371-9354-3de90699e135"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ],
      "source": [
        "F.binary_cross_entropy(hypothesis, y_train) # log cost 함수와 동일함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r901yx9BZC45"
      },
      "source": [
        "## Training with Low-level Binary Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "K0in_t-bZC46"
      },
      "outputs": [],
      "source": [
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rbp808_SZC47",
        "outputId": "b4ea4b87-294e-445f-f541-3f4c7dc05b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/5000 Cost: 0.693147\n",
            "Epoch  500/5000 Cost: 0.037261\n",
            "Epoch 1000/5000 Cost: 0.019852\n",
            "Epoch 1500/5000 Cost: 0.013562\n",
            "Epoch 2000/5000 Cost: 0.010305\n",
            "Epoch 2500/5000 Cost: 0.008311\n",
            "Epoch 3000/5000 Cost: 0.006965\n",
            "Epoch 3500/5000 Cost: 0.005994\n",
            "Epoch 4000/5000 Cost: 0.005260\n",
            "Epoch 4500/5000 Cost: 0.004687\n",
            "Epoch 5000/5000 Cost: 0.004227\n"
          ]
        }
      ],
      "source": [
        "# 모델 초기화\n",
        "W = torch.zeros((2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "# optimizer 설정 경사하강법\n",
        "optimizer = optim.SGD([W, b], lr=1)\n",
        "\n",
        "nb_epochs = 5000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # Cost 계산\n",
        "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
        "    cost = -(y_train * torch.log(hypothesis) + \n",
        "             (1 - y_train) * torch.log(1 - hypothesis)).mean() # cost 함수\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 500 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9k9H31lZC4-"
      },
      "source": [
        "## Training with `F.binary_cross_entropy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "520rXXVQZC4_",
        "outputId": "51338bac-9961-46b5-8fa3-01cbb2c7e353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/10000 Cost: 0.693147\n",
            "Epoch  100/10000 Cost: 0.134722\n",
            "Epoch  200/10000 Cost: 0.080643\n",
            "Epoch  300/10000 Cost: 0.057900\n",
            "Epoch  400/10000 Cost: 0.045300\n",
            "Epoch  500/10000 Cost: 0.037261\n",
            "Epoch  600/10000 Cost: 0.031672\n",
            "Epoch  700/10000 Cost: 0.027556\n",
            "Epoch  800/10000 Cost: 0.024394\n",
            "Epoch  900/10000 Cost: 0.021888\n",
            "Epoch 1000/10000 Cost: 0.019852\n",
            "Epoch 1100/10000 Cost: 0.018165\n",
            "Epoch 1200/10000 Cost: 0.016743\n",
            "Epoch 1300/10000 Cost: 0.015528\n",
            "Epoch 1400/10000 Cost: 0.014479\n",
            "Epoch 1500/10000 Cost: 0.013562\n",
            "Epoch 1600/10000 Cost: 0.012755\n",
            "Epoch 1700/10000 Cost: 0.012039\n",
            "Epoch 1800/10000 Cost: 0.011400\n",
            "Epoch 1900/10000 Cost: 0.010825\n",
            "Epoch 2000/10000 Cost: 0.010305\n",
            "Epoch 2100/10000 Cost: 0.009833\n",
            "Epoch 2200/10000 Cost: 0.009403\n",
            "Epoch 2300/10000 Cost: 0.009008\n",
            "Epoch 2400/10000 Cost: 0.008646\n",
            "Epoch 2500/10000 Cost: 0.008311\n",
            "Epoch 2600/10000 Cost: 0.008002\n",
            "Epoch 2700/10000 Cost: 0.007715\n",
            "Epoch 2800/10000 Cost: 0.007447\n",
            "Epoch 2900/10000 Cost: 0.007198\n",
            "Epoch 3000/10000 Cost: 0.006965\n",
            "Epoch 3100/10000 Cost: 0.006746\n",
            "Epoch 3200/10000 Cost: 0.006541\n",
            "Epoch 3300/10000 Cost: 0.006348\n",
            "Epoch 3400/10000 Cost: 0.006166\n",
            "Epoch 3500/10000 Cost: 0.005994\n",
            "Epoch 3600/10000 Cost: 0.005831\n",
            "Epoch 3700/10000 Cost: 0.005677\n",
            "Epoch 3800/10000 Cost: 0.005531\n",
            "Epoch 3900/10000 Cost: 0.005392\n",
            "Epoch 4000/10000 Cost: 0.005260\n",
            "Epoch 4100/10000 Cost: 0.005135\n",
            "Epoch 4200/10000 Cost: 0.005015\n",
            "Epoch 4300/10000 Cost: 0.004901\n",
            "Epoch 4400/10000 Cost: 0.004792\n",
            "Epoch 4500/10000 Cost: 0.004687\n",
            "Epoch 4600/10000 Cost: 0.004587\n",
            "Epoch 4700/10000 Cost: 0.004491\n",
            "Epoch 4800/10000 Cost: 0.004400\n",
            "Epoch 4900/10000 Cost: 0.004311\n",
            "Epoch 5000/10000 Cost: 0.004227\n",
            "Epoch 5100/10000 Cost: 0.004145\n",
            "Epoch 5200/10000 Cost: 0.004067\n",
            "Epoch 5300/10000 Cost: 0.003991\n",
            "Epoch 5400/10000 Cost: 0.003919\n",
            "Epoch 5500/10000 Cost: 0.003848\n",
            "Epoch 5600/10000 Cost: 0.003781\n",
            "Epoch 5700/10000 Cost: 0.003715\n",
            "Epoch 5800/10000 Cost: 0.003652\n",
            "Epoch 5900/10000 Cost: 0.003591\n",
            "Epoch 6000/10000 Cost: 0.003532\n",
            "Epoch 6100/10000 Cost: 0.003475\n",
            "Epoch 6200/10000 Cost: 0.003420\n",
            "Epoch 6300/10000 Cost: 0.003367\n",
            "Epoch 6400/10000 Cost: 0.003315\n",
            "Epoch 6500/10000 Cost: 0.003264\n",
            "Epoch 6600/10000 Cost: 0.003216\n",
            "Epoch 6700/10000 Cost: 0.003168\n",
            "Epoch 6800/10000 Cost: 0.003122\n",
            "Epoch 6900/10000 Cost: 0.003078\n",
            "Epoch 7000/10000 Cost: 0.003034\n",
            "Epoch 7100/10000 Cost: 0.002992\n",
            "Epoch 7200/10000 Cost: 0.002951\n",
            "Epoch 7300/10000 Cost: 0.002911\n",
            "Epoch 7400/10000 Cost: 0.002872\n",
            "Epoch 7500/10000 Cost: 0.002834\n",
            "Epoch 7600/10000 Cost: 0.002797\n",
            "Epoch 7700/10000 Cost: 0.002761\n",
            "Epoch 7800/10000 Cost: 0.002726\n",
            "Epoch 7900/10000 Cost: 0.002692\n",
            "Epoch 8000/10000 Cost: 0.002659\n",
            "Epoch 8100/10000 Cost: 0.002627\n",
            "Epoch 8200/10000 Cost: 0.002595\n",
            "Epoch 8300/10000 Cost: 0.002564\n",
            "Epoch 8400/10000 Cost: 0.002534\n",
            "Epoch 8500/10000 Cost: 0.002504\n",
            "Epoch 8600/10000 Cost: 0.002475\n",
            "Epoch 8700/10000 Cost: 0.002447\n",
            "Epoch 8800/10000 Cost: 0.002420\n",
            "Epoch 8900/10000 Cost: 0.002393\n",
            "Epoch 9000/10000 Cost: 0.002366\n",
            "Epoch 9100/10000 Cost: 0.002341\n",
            "Epoch 9200/10000 Cost: 0.002316\n",
            "Epoch 9300/10000 Cost: 0.002291\n",
            "Epoch 9400/10000 Cost: 0.002267\n",
            "Epoch 9500/10000 Cost: 0.002243\n",
            "Epoch 9600/10000 Cost: 0.002220\n",
            "Epoch 9700/10000 Cost: 0.002197\n",
            "Epoch 9800/10000 Cost: 0.002175\n",
            "Epoch 9900/10000 Cost: 0.002153\n",
            "Epoch 10000/10000 Cost: 0.002132\n"
          ]
        }
      ],
      "source": [
        "# 모델 초기화\n",
        "W = torch.zeros((2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=1)\n",
        "\n",
        "nb_epochs = 10000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # Cost 계산\n",
        "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
        "    cost = F.binary_cross_entropy(hypothesis, y_train) # cost 함수\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj-lErN1ZC5B"
      },
      "source": [
        "## Loading Real Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "ULVoZpJ4ZC5B"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "tLIJTw2hHDAZ",
        "outputId": "c384ee75-64e0-4429-f2fb-1c06d0435425"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d26527a4-2fb5-4dcf-b6cc-942ed258bd86\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d26527a4-2fb5-4dcf-b6cc-942ed258bd86\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data-03-diabetes.csv to data-03-diabetes.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data-03-diabetes.csv': b'-0.294118,0.487437,0.180328,-0.292929,0,0.00149028,-0.53117,-0.0333333,0\\n-0.882353,-0.145729,0.0819672,-0.414141,0,-0.207153,-0.766866,-0.666667,1\\n-0.0588235,0.839196,0.0491803,0,0,-0.305514,-0.492741,-0.633333,0\\n-0.882353,-0.105528,0.0819672,-0.535354,-0.777778,-0.162444,-0.923997,0,1\\n0,0.376884,-0.344262,-0.292929,-0.602837,0.28465,0.887276,-0.6,0\\n-0.411765,0.165829,0.213115,0,0,-0.23696,-0.894962,-0.7,1\\n-0.647059,-0.21608,-0.180328,-0.353535,-0.791962,-0.0760059,-0.854825,-0.833333,0\\n0.176471,0.155779,0,0,0,0.052161,-0.952178,-0.733333,1\\n-0.764706,0.979899,0.147541,-0.0909091,0.283688,-0.0909091,-0.931682,0.0666667,0\\n-0.0588235,0.256281,0.57377,0,0,0,-0.868488,0.1,0\\n-0.529412,0.105528,0.508197,0,0,0.120715,-0.903501,-0.7,1\\n0.176471,0.688442,0.213115,0,0,0.132638,-0.608027,-0.566667,0\\n0.176471,0.396985,0.311475,0,0,-0.19225,0.163962,0.2,1\\n-0.882353,0.899497,-0.0163934,-0.535354,1,-0.102832,-0.726729,0.266667,0\\n-0.176471,0.00502513,0,0,0,-0.105812,-0.653288,-0.633333,0\\n0,0.18593,0.377049,-0.0505051,-0.456265,0.365127,-0.596072,-0.666667,0\\n-0.176471,0.0753769,0.213115,0,0,-0.117735,-0.849701,-0.666667,0\\n-0.882353,0.0351759,-0.508197,-0.232323,-0.803783,0.290611,-0.910333,-0.6,1\\n-0.882353,0.155779,0.147541,-0.393939,-0.77305,0.0312965,-0.614859,-0.633333,0\\n-0.647059,0.266332,0.442623,-0.171717,-0.444444,0.171386,-0.465414,-0.8,1\\n-0.0588235,-0.00502513,0.377049,0,0,0.0551417,-0.735269,-0.0333333,1\\n-0.176471,0.969849,0.47541,0,0,0.186289,-0.681469,-0.333333,0\\n0.0588235,0.19598,0.311475,-0.292929,0,-0.135618,-0.842015,-0.733333,0\\n0.176471,0.256281,0.147541,-0.474747,-0.728132,-0.0730253,-0.891546,-0.333333,0\\n-0.176471,0.477387,0.245902,0,0,0.174367,-0.847139,-0.266667,0\\n-0.882353,-0.0251256,0.0819672,-0.69697,-0.669031,-0.308495,-0.650726,-0.966667,1\\n0.529412,0.457286,0.344262,-0.616162,-0.739953,-0.338301,-0.857387,0.2,1\\n-0.411765,0.175879,0.508197,0,0,0.0163934,-0.778822,-0.433333,1\\n-0.411765,0.0954774,0.229508,-0.474747,0,0.0730254,-0.600342,0.3,1\\n-0.647059,0.58794,0.245902,-0.272727,-0.420804,-0.0581222,-0.33988,-0.766667,0\\n-0.647059,-0.115578,-0.0491803,-0.777778,-0.87234,-0.260805,-0.838599,-0.966667,1\\n-0.294118,-0.0753769,0.508197,0,0,-0.406855,-0.906063,-0.766667,1\\n0.176471,0.226131,0.278689,-0.373737,0,-0.177347,-0.629377,-0.2,1\\n-0.529412,0.0351759,-0.0163934,-0.333333,-0.546099,-0.28465,-0.241674,-0.6,1\\n0.294118,0.386935,0.245902,0,0,-0.0104321,-0.707942,-0.533333,1\\n0.0588235,0.0251256,0.245902,-0.252525,0,-0.019374,-0.498719,-0.166667,0\\n-0.764706,-0.0954774,0.114754,-0.151515,0,0.138599,-0.637062,-0.8,0\\n-0.529412,0.115578,0.180328,-0.0505051,-0.510638,0.105812,0.12041,0.166667,0\\n-0.647059,0.809045,0.0491803,-0.494949,-0.834515,0.0134128,-0.835184,-0.833333,1\\n-0.176471,0.336683,0.377049,0,0,0.198212,-0.472246,-0.466667,1\\n-0.176471,0.0653266,0.508197,-0.636364,0,-0.323398,-0.865927,-0.1,1\\n0.0588235,0.718593,0.803279,-0.515152,-0.432624,0.353204,-0.450897,0.1,0\\n-0.176471,0.59799,0.0491803,0,0,-0.183308,-0.815542,-0.366667,1\\n0,0.809045,0.0819672,-0.212121,0,0.251863,0.549957,-0.866667,0\\n-0.882353,0.467337,-0.0819672,0,0,-0.114754,-0.58497,-0.733333,1\\n-0.764706,-0.286432,0.147541,-0.454545,0,-0.165425,-0.566183,-0.966667,1\\n-0.176471,0.0351759,0.0819672,-0.353535,0,0.165425,-0.772844,-0.666667,0\\n-0.176471,0.0552764,0,0,0,0,-0.806149,-0.9,1\\n-0.882353,0.0351759,0.311475,-0.777778,-0.806147,-0.421759,-0.64731,-0.966667,1\\n-0.882353,0.0150754,-0.180328,-0.69697,-0.914894,-0.278688,-0.617421,-0.833333,1\\n-0.411765,-0.115578,0.0819672,-0.575758,-0.945626,-0.272727,-0.774552,-0.7,1\\n-0.0588235,0.768844,0.47541,-0.313131,-0.29078,0.004471,-0.667805,0.233333,0\\n-0.176471,0.507538,0.0819672,-0.151515,-0.191489,0.0342773,-0.453459,-0.3,1\\n-0.882353,-0.266332,-0.180328,-0.79798,0,-0.314456,-0.854825,0,1\\n-0.176471,0.879397,0.114754,-0.212121,-0.281324,0.123696,-0.849701,-0.333333,0\\n0,0.00502513,0.442623,0.212121,-0.739953,0.394933,-0.24509,-0.666667,1\\n0,0.467337,0.344262,0,0,0.207154,0.454313,-0.233333,1\\n0,0.0552764,0.0491803,-0.171717,-0.664303,0.23696,-0.918873,-0.966667,1\\n-0.764706,-0.155779,0,0,0,0,-0.807003,0,1\\n-0.0588235,0.336683,0.180328,0,0,-0.019374,-0.836038,-0.4,0\\n-0.411765,-0.557789,0.0163934,0,0,-0.254843,-0.565329,-0.5,1\\n-0.764706,0.417085,-0.0491803,-0.313131,-0.6974,-0.242921,-0.469684,-0.9,1\\n-0.176471,0.145729,0.0819672,0,0,-0.0223547,-0.846285,-0.3,0\\n-0.411765,-0.00502513,0.213115,-0.454545,0,-0.135618,-0.893254,-0.633333,1\\n0,0.0954774,0.442623,-0.393939,0,-0.0312965,-0.336465,-0.433333,0\\n-0.764706,0.0954774,0.508197,0,0,0.272727,-0.345004,0.1,1\\n-0.882353,-0.0452261,0.0819672,-0.737374,-0.910165,-0.415797,-0.781383,-0.866667,1\\n-0.529412,0.467337,0.393443,-0.454545,-0.763593,-0.138599,-0.905209,-0.8,1\\n-0.764706,0.00502513,0.0819672,-0.59596,-0.787234,-0.019374,-0.326217,-0.766667,0\\n-0.411765,0.396985,0.0491803,-0.292929,-0.669031,-0.147541,-0.715628,-0.833333,1\\n0.529412,0.266332,0.47541,0,0,0.293592,-0.568745,-0.3,0\\n-0.529412,0.296482,0.409836,-0.59596,-0.361702,0.0461997,-0.869342,-0.933333,1\\n-0.882353,-0.20603,0.229508,-0.393939,0,-0.0461997,-0.728437,-0.966667,1\\n-0.882353,0,-0.213115,-0.59596,0,-0.263785,-0.947054,-0.966667,1\\n-0.176471,-0.376884,0.278689,0,0,-0.028316,-0.732707,-0.333333,1\\n-0.411765,-0.0452261,0.180328,-0.333333,0,0.123696,-0.75064,-0.8,1\\n0,0.316583,0,0,0,0.28763,-0.836038,-0.833333,0\\n-0.764706,0.125628,0.0819672,-0.555556,0,-0.254843,-0.804441,-0.9,1\\n-0.647059,0.135678,-0.278689,-0.737374,0,-0.33234,-0.947054,-0.966667,1\\n-0.764706,-0.256281,0,0,0,0,-0.979505,-0.966667,1\\n-0.176471,-0.165829,0.278689,-0.474747,-0.832151,-0.126677,-0.411614,-0.5,1\\n0,0.0150754,0.0655738,-0.434343,0,-0.266766,-0.864219,-0.966667,1\\n-0.411765,0.376884,0.770492,0,0,0.454545,-0.872758,-0.466667,0\\n-0.764706,0.105528,0.213115,-0.414141,-0.704492,-0.0342771,-0.470538,-0.8,1\\n0.529412,0.0653266,0.180328,0.0909091,0,0.0909091,-0.914603,-0.2,1\\n-0.764706,0.00502513,0.114754,-0.494949,-0.832151,0.147541,-0.789923,-0.833333,1\\n0.764706,0.366834,0.147541,-0.353535,-0.739953,0.105812,-0.935952,-0.266667,0\\n-0.882353,0.0753769,0.114754,-0.616162,0,-0.210134,-0.925705,-0.9,1\\n-0.882353,-0.19598,-0.0983607,0,0,-0.4307,-0.846285,0,1\\n-0.529412,0.236181,0.311475,-0.69697,-0.583924,-0.0461997,-0.688301,-0.566667,1\\n-0.176471,-0.18593,0.278689,-0.191919,-0.886525,0.391952,-0.843723,-0.3,1\\n-0.529412,0.346734,0.180328,0,0,-0.290611,-0.83006,0.3,0\\n-0.764706,0.427136,0.344262,-0.636364,-0.8487,-0.263785,-0.416738,0,1\\n-0.294118,0.447236,0.180328,-0.454545,-0.460993,0.0104323,-0.848847,-0.366667,1\\n-0.764706,-0.0753769,0.0163934,-0.434343,0,-0.0581222,-0.955594,-0.9,1\\n-0.882353,-0.286432,-0.213115,-0.636364,-0.820331,-0.391952,-0.790777,-0.966667,1\\n-0.294118,-0.0653266,-0.180328,-0.393939,-0.8487,-0.14456,-0.762596,-0.933333,1\\n-0.882353,0.226131,0.47541,0.030303,-0.479905,0.481371,-0.789069,-0.666667,0\\n-0.882353,0.638191,0.180328,0,0,0.162444,-0.0230572,-0.6,0\\n-0.882353,0.517588,-0.0163934,0,0,-0.222057,-0.913749,-0.966667,1\\n0,0.256281,0.57377,0,0,-0.329359,-0.842869,0,1\\n-0.882353,-0.18593,0.180328,-0.636364,-0.905437,-0.207153,-0.824936,-0.9,1\\n-0.764706,-0.145729,0.0655738,0,0,0.180328,-0.272417,-0.8,1\\n-0.882353,0.266332,-0.0819672,-0.414141,-0.640662,-0.14456,-0.382579,0,1\\n-0.882353,-0.0351759,1,0,0,-0.33234,-0.889838,-0.8,1\\n-0.529412,0.447236,-0.0491803,-0.434343,-0.669031,-0.120715,-0.82152,-0.466667,1\\n-0.647059,-0.165829,-0.0491803,-0.373737,-0.957447,0.0223547,-0.779675,-0.866667,1\\n0,-0.0452261,0.393443,-0.494949,-0.914894,0.114754,-0.855679,-0.9,0\\n-0.647059,0.718593,0.180328,-0.333333,-0.680851,-0.00745157,-0.89667,-0.9,0\\n-0.0588235,0.557789,0.0163934,-0.474747,0.170213,0.0134128,-0.602904,-0.166667,0\\n-0.882353,-0.105528,0.245902,-0.313131,-0.91253,-0.0700447,-0.902647,-0.933333,1\\n-0.529412,-0.236181,0.0163934,0,0,0.0134128,-0.732707,-0.866667,1\\n-0.176471,0.60804,-0.114754,-0.353535,-0.586288,-0.0909091,-0.564475,-0.4,0\\n-0.529412,0.467337,0.508197,0,0,-0.0700447,-0.606319,0.333333,0\\n-0.411765,0.246231,0.213115,0,0,0.0134128,-0.878736,-0.433333,0\\n-0.411765,-0.21608,-0.213115,0,0,0.004471,-0.508113,-0.866667,1\\n-0.529412,-0.0251256,-0.0163934,-0.535354,0,-0.159463,-0.688301,-0.966667,1\\n-0.529412,-0.00502513,0.245902,-0.69697,-0.879433,-0.308495,-0.876174,0,1\\n0,0.628141,0.245902,0.131313,-0.763593,0.585693,-0.418446,-0.866667,0\\n-0.294118,0.115578,0.0491803,-0.212121,0,0.0193741,-0.844577,-0.9,1\\n-0.764706,0.0753769,0.213115,-0.393939,-0.763593,0.00149028,-0.721605,-0.933333,1\\n-0.411765,0.326633,0.311475,0,0,-0.201192,-0.907771,0.6,1\\n0,0.135678,0.245902,0,0,-0.00745157,-0.829206,-0.933333,0\\n-0.882353,-0.115578,-0.508197,-0.151515,-0.765957,0.639344,-0.64304,-0.833333,0\\n-0.647059,0.20603,0.147541,-0.393939,-0.680851,0.278689,-0.680615,-0.7,1\\n-0.882353,0.18593,-0.0491803,-0.272727,-0.777778,-0.00745157,-0.843723,-0.933333,1\\n-0.882353,0.175879,0.442623,-0.515152,-0.65721,0.028316,-0.722459,-0.366667,0\\n0,0.0552764,0.377049,0,0,-0.168405,-0.433817,0.366667,0\\n-0.529412,0.738693,0.147541,-0.717172,-0.602837,-0.114754,-0.758326,-0.6,0\\n0.0588235,0.226131,-0.0819672,0,0,-0.00745157,-0.115286,-0.6,0\\n-0.647059,0.708543,0.0491803,-0.252525,-0.468085,0.028316,-0.762596,-0.7,0\\n-0.0588235,-0.155779,0.213115,-0.373737,0,0.14158,-0.676345,-0.4,1\\n-0.764706,-0.0351759,0.114754,-0.737374,-0.884161,-0.371088,-0.514091,-0.833333,1\\n-0.764706,0.256281,-0.0163934,-0.59596,-0.669031,0.00745157,-0.99146,-0.666667,1\\n0,0.00502513,0.147541,-0.474747,-0.881797,-0.0819672,-0.556789,0,1\\n0,-0.0653266,-0.0163934,-0.494949,-0.782506,-0.14456,-0.612297,-0.966667,1\\n0,0.296482,0.311475,0,0,-0.0700447,-0.466268,-0.733333,1\\n-0.411765,0.0552764,0.180328,-0.414141,-0.231678,0.0998511,-0.930828,-0.766667,1\\n-0.647059,0.286432,0.278689,0,0,-0.371088,-0.837746,0.133333,1\\n-0.411765,0.0653266,0.344262,-0.393939,0,0.177347,-0.822374,-0.433333,1\\n-0.764706,0.0854271,-0.147541,-0.474747,-0.851064,-0.0312965,-0.795047,-0.966667,1\\n0.176471,0.0854271,0.0819672,0,0,-0.0342771,-0.83433,-0.3,0\\n-0.529412,0.547739,0.0163934,-0.373737,-0.328605,-0.0223547,-0.864219,-0.933333,1\\n0,0.0251256,0.229508,-0.535354,0,0,-0.578138,0,1\\n0.0588235,-0.427136,0.311475,-0.252525,0,-0.0223547,-0.984629,-0.333333,1\\n-0.764706,0.0653266,0.0491803,-0.292929,-0.718676,-0.0909091,0.12895,-0.566667,1\\n-0.411765,0.477387,0.278689,0,0,0.004471,-0.880444,0.466667,1\\n-0.764706,-0.0954774,0.147541,-0.656566,0,-0.186289,-0.994022,-0.966667,1\\n-0.882353,0.366834,0.213115,0.010101,-0.51773,0.114754,-0.725875,-0.9,1\\n-0.529412,0.145729,0.0655738,0,0,-0.347243,-0.697694,-0.466667,1\\n0.0588235,0.567839,0.409836,-0.434343,-0.63357,0.0223547,-0.0512383,-0.3,0\\n-0.882353,0.537688,0.344262,-0.151515,0.146572,0.210134,-0.479932,-0.933333,1\\n-0.0588235,0.889447,0.278689,0,0,0.42772,-0.949616,-0.266667,0\\n-0.176471,0.527638,0.442623,-0.111111,0,0.490313,-0.778822,-0.5,0\\n-0.764706,-0.00502513,-0.147541,-0.69697,-0.777778,-0.266766,-0.52263,0,1\\n-0.882353,0.0954774,-0.0819672,-0.575758,-0.680851,-0.248882,-0.355252,-0.933333,1\\n-0.764706,-0.115578,0.213115,-0.616162,-0.874704,-0.135618,-0.87105,-0.966667,1\\n1,0.638191,0.180328,-0.171717,-0.730496,0.219076,-0.368915,-0.133333,0\\n-0.529412,0.517588,0.47541,-0.232323,0,-0.114754,-0.815542,-0.5,1\\n-0.176471,0.0251256,0.213115,-0.191919,-0.751773,0.108793,-0.8924,-0.2,1\\n0,0.145729,0.311475,-0.313131,-0.326241,0.317437,-0.923997,-0.8,1\\n-0.764706,0.00502513,0.0491803,-0.535354,0,-0.114754,-0.752348,0,1\\n0,0.316583,0.442623,0,0,-0.0581222,-0.432109,-0.633333,0\\n-0.294118,0.0452261,0.213115,-0.636364,-0.631206,-0.108793,-0.450043,-0.333333,0\\n-0.647059,0.487437,0.0819672,-0.494949,0,-0.0312965,-0.847993,-0.966667,1\\n-0.529412,0.20603,0.114754,0,0,-0.117735,-0.461144,-0.566667,1\\n-0.529412,0.105528,0.0819672,0,0,-0.0491803,-0.664389,-0.733333,1\\n-0.647059,0.115578,0.47541,-0.757576,-0.815603,-0.153502,-0.643894,-0.733333,1\\n-0.294118,0.0251256,0.344262,0,0,-0.0819672,-0.912895,-0.5,0\\n-0.294118,0.346734,0.147541,-0.535354,-0.692671,0.0551417,-0.603757,-0.733333,0\\n-0.764706,-0.125628,0,-0.535354,0,-0.138599,-0.40649,-0.866667,1\\n-0.882353,-0.20603,-0.0163934,-0.151515,-0.886525,0.296572,-0.487617,-0.933333,1\\n-0.764706,-0.246231,0.0491803,-0.515152,-0.869976,-0.114754,-0.75064,-0.6,1\\n-0.0588235,0.798995,0.180328,-0.151515,-0.692671,-0.0253353,-0.452605,-0.5,0\\n-0.294118,-0.145729,0.278689,0,0,-0.0700447,-0.740393,-0.3,1\\n0,0.296482,0.803279,-0.0707071,-0.692671,1,-0.794193,-0.833333,0\\n-0.411765,0.437186,0.278689,0,0,0.341282,-0.904355,-0.133333,1\\n-0.411765,0.306533,0.344262,0,0,0.165425,-0.250213,-0.466667,0\\n-0.294118,-0.125628,0.311475,0,0,-0.308495,-0.994876,-0.633333,1\\n0,0.19598,0.0491803,-0.636364,-0.782506,0.0402385,-0.447481,-0.933333,1\\n-0.882353,0,0.213115,-0.59596,-0.945626,-0.174367,-0.811272,0,1\\n-0.411765,-0.266332,-0.0163934,0,0,-0.201192,-0.837746,-0.8,1\\n-0.529412,0.417085,0.213115,0,0,-0.177347,-0.858241,-0.366667,1\\n-0.176471,0.949749,0.114754,-0.434343,0,0.0700448,-0.430401,-0.333333,0\\n-0.0588235,0.819095,0.114754,-0.272727,0.170213,-0.102832,-0.541418,0.3,0\\n-0.882353,0.286432,0.606557,-0.171717,-0.862884,-0.0461997,0.0614859,-0.6,0\\n-0.0588235,0.0954774,0.245902,-0.212121,-0.730496,-0.168405,-0.520068,-0.666667,0\\n-0.411765,0.396985,0.311475,-0.292929,-0.621749,-0.0581222,-0.758326,-0.866667,0\\n-0.647059,0.115578,0.0163934,0,0,-0.326379,-0.945346,0,1\\n0.0588235,0.236181,0.147541,-0.111111,-0.777778,-0.0134128,-0.747225,-0.366667,1\\n-0.176471,0.59799,0.0819672,0,0,-0.0938897,-0.739539,-0.5,0\\n0.294118,0.356784,0,0,0,0.558867,-0.573015,-0.366667,0\\n-0.0588235,-0.145729,-0.0983607,-0.59596,0,-0.272727,-0.95047,-0.3,1\\n-0.411765,0.58794,0.377049,-0.171717,-0.503546,0.174367,-0.729291,-0.733333,0\\n-0.882353,0.0552764,-0.0491803,0,0,-0.275708,-0.906917,0,1\\n-0.647059,0.0753769,0.0163934,-0.737374,-0.886525,-0.317437,-0.487617,-0.933333,0\\n-0.529412,0.0954774,0.0491803,-0.111111,-0.765957,0.0372578,-0.293766,-0.833333,0\\n-0.529412,0.487437,-0.0163934,-0.454545,-0.248227,-0.0789866,-0.938514,-0.733333,0\\n0,0.135678,0.311475,-0.676768,0,-0.0760059,-0.320239,0,1\\n-0.882353,0.386935,0.344262,0,0,0.195231,-0.865073,-0.766667,1\\n0,0.0854271,0.114754,-0.59596,0,-0.186289,-0.394535,-0.633333,1\\n-0.764706,-0.00502513,0.147541,-0.676768,-0.895981,-0.391952,-0.865927,-0.8,1\\n-0.294118,0.0351759,0.180328,-0.353535,-0.550827,0.123696,-0.789923,0.133333,1\\n-0.411765,0.115578,0.180328,-0.434343,0,-0.28763,-0.719044,-0.8,1\\n-0.0588235,0.969849,0.245902,-0.414141,-0.338061,0.117735,-0.549957,0.2,0\\n-0.411765,0.628141,0.704918,0,0,0.123696,-0.93766,0.0333333,0\\n-0.882353,-0.0351759,0.0491803,-0.454545,-0.794326,-0.0104321,-0.819812,0,1\\n-0.176471,0.849246,0.377049,-0.333333,0,0.0581222,-0.76345,-0.333333,0\\n-0.764706,-0.18593,-0.0163934,-0.555556,0,-0.174367,-0.818958,-0.866667,1\\n0,0.477387,0.393443,0.0909091,0,0.275708,-0.746371,-0.9,1\\n-0.176471,0.798995,0.557377,-0.373737,0,0.0193741,-0.926558,0.3,1\\n0,0.407035,0.0655738,-0.474747,-0.692671,0.269747,-0.698548,-0.9,0\\n0.0588235,0.125628,0.344262,-0.353535,-0.586288,0.0193741,-0.844577,-0.5,0\\n0.411765,0.517588,0.147541,-0.191919,-0.359338,0.245902,-0.432963,-0.433333,0\\n-0.411765,0.0954774,0.0163934,-0.171717,-0.695035,0.0670641,-0.627669,-0.866667,0\\n-0.294118,0.256281,0.114754,-0.393939,-0.716312,-0.105812,-0.670367,-0.633333,1\\n-0.411765,-0.145729,0.213115,-0.555556,0,-0.135618,-0.0213493,-0.633333,0\\n-0.411765,0.125628,0.0819672,0,0,0.126677,-0.843723,-0.333333,0\\n0,0.778894,-0.0163934,-0.414141,0.130024,0.0312965,-0.151153,0,0\\n-0.764706,0.58794,0.47541,0,0,-0.0581222,-0.379163,0.5,0\\n-0.176471,0.19598,0,0,0,-0.248882,-0.88813,-0.466667,1\\n-0.176471,0.427136,-0.0163934,-0.333333,-0.550827,-0.14158,-0.479932,0.333333,1\\n-0.882353,0.00502513,0.0819672,-0.69697,-0.867612,-0.296572,-0.497865,-0.833333,1\\n-0.882353,-0.125628,0.278689,-0.454545,-0.92435,0.0312965,-0.980359,-0.966667,1\\n0,0.0150754,0.245902,0,0,0.0640835,-0.897523,-0.833333,1\\n-0.647059,0.628141,-0.147541,-0.232323,0,0.108793,-0.509821,-0.9,0\\n-0.529412,0.979899,0.147541,-0.212121,0.758865,0.0938898,0.922289,-0.666667,1\\n0,0.175879,0.311475,-0.373737,-0.874704,0.347243,-0.990606,-0.9,1\\n-0.529412,0.427136,0.409836,0,0,0.311475,-0.515798,-0.966667,0\\n-0.294118,0.346734,0.311475,-0.252525,-0.125296,0.377049,-0.863365,-0.166667,0\\n-0.882353,-0.20603,0.311475,-0.494949,-0.91253,-0.242921,-0.568745,-0.966667,1\\n-0.529412,0.226131,0.114754,0,0,0.0432191,-0.730145,-0.733333,1\\n-0.647059,-0.256281,0.114754,-0.434343,-0.893617,-0.114754,-0.816396,-0.933333,1\\n-0.529412,0.718593,0.180328,0,0,0.299553,-0.657558,-0.833333,0\\n0,0.798995,0.47541,-0.454545,0,0.314456,-0.480786,-0.933333,0\\n0.0588235,0.648241,0.377049,-0.575758,0,-0.0819672,-0.35696,-0.633333,0\\n0,0.0452261,0.245902,0,0,-0.451565,-0.569599,-0.8,1\\n-0.882353,-0.0854271,0.0491803,-0.515152,0,-0.129657,-0.902647,0,1\\n-0.529412,-0.0854271,0.147541,-0.353535,-0.791962,-0.0134128,-0.685739,-0.966667,1\\n-0.647059,0.396985,-0.114754,0,0,-0.23696,-0.723313,-0.966667,0\\n-0.294118,0.19598,-0.180328,-0.555556,-0.583924,-0.19225,0.058924,-0.6,0\\n-0.764706,0.467337,0.245902,-0.292929,-0.541371,0.138599,-0.785653,-0.733333,1\\n0.0588235,0.849246,0.393443,-0.69697,0,-0.105812,-0.030743,-0.0666667,0\\n0.176471,0.226131,0.114754,0,0,-0.0700447,-0.846285,-0.333333,1\\n0,0.658291,0.47541,-0.333333,0.607565,0.558867,-0.701964,-0.933333,1\\n0.0588235,0.246231,0.147541,-0.333333,-0.0496454,0.0551417,-0.82579,-0.566667,1\\n-0.882353,0.115578,0.409836,-0.616162,0,-0.102832,-0.944492,-0.933333,1\\n0.0588235,0.0653266,-0.147541,0,0,-0.0700447,-0.742101,-0.3,1\\n-0.764706,0.296482,0.377049,0,0,-0.165425,-0.824082,-0.8,1\\n-0.764706,-0.0954774,0.311475,-0.717172,-0.869976,-0.272727,-0.853971,-0.9,1\\n0,-0.135678,0.114754,-0.353535,0,0.0670641,-0.863365,-0.866667,1\\n0.411765,-0.0753769,0.0163934,-0.858586,-0.390071,-0.177347,-0.275833,-0.233333,0\\n-0.882353,0.135678,0.0491803,-0.292929,0,0.00149028,-0.602904,0,0\\n-0.647059,0.115578,-0.0819672,-0.212121,0,-0.102832,-0.590948,-0.7,1\\n-0.764706,0.145729,0.114754,-0.555556,0,-0.14456,-0.988044,-0.866667,1\\n-0.882353,0.939698,-0.180328,-0.676768,-0.113475,-0.228018,-0.507259,-0.9,1\\n-0.647059,0.919598,0.114754,-0.69697,-0.692671,-0.0789866,-0.811272,-0.566667,1\\n-0.647059,0.417085,0,0,0,-0.105812,-0.416738,-0.8,0\\n-0.529412,-0.0452261,0.147541,-0.353535,0,-0.0432191,-0.54398,-0.9,1\\n-0.647059,0.427136,0.311475,-0.69697,0,-0.0342771,-0.895816,0.4,1\\n-0.529412,0.236181,0.0163934,0,0,-0.0461997,-0.873612,-0.533333,0\\n-0.411765,-0.0351759,0.213115,-0.636364,-0.841608,0.00149028,-0.215201,-0.266667,1\\n0,0.386935,0,0,0,0.0819672,-0.269855,-0.866667,0\\n-0.764706,0.286432,0.0491803,-0.151515,0,0.19225,-0.126388,-0.9,1\\n0,0.0251256,-0.147541,0,0,-0.251863,0,0,1\\n-0.764706,0.467337,0,0,0,-0.180328,-0.861657,-0.766667,0\\n0.176471,0.0150754,0.409836,-0.252525,0,0.359165,-0.0964987,-0.433333,0\\n-0.764706,0.0854271,0.0163934,-0.353535,-0.867612,-0.248882,-0.957301,0,1\\n-0.647059,0.226131,0.278689,0,0,-0.314456,-0.849701,-0.366667,1\\n-0.882353,-0.286432,0.278689,0.010101,-0.893617,-0.0104321,-0.706234,0,1\\n0.529412,0.0653266,0.147541,0,0,0.0193741,-0.852263,0.0333333,1\\n-0.764706,0.00502513,0.147541,0.0505051,-0.865248,0.207154,-0.488471,-0.866667,1\\n-0.176471,0.0653266,-0.0163934,-0.515152,0,-0.210134,-0.813834,-0.733333,0\\n0,0.0452261,0.0491803,-0.535354,-0.725768,-0.171386,-0.678907,-0.933333,1\\n-0.411765,0.145729,0.213115,0,0,-0.257824,-0.431255,0.2,1\\n-0.764706,0.0854271,0.0163934,-0.79798,-0.34279,-0.245902,-0.314261,-0.966667,1\\n0,0.467337,0.147541,0,0,0.129657,-0.781383,-0.766667,0\\n0.176471,0.296482,0.245902,-0.434343,-0.711584,0.0700448,-0.827498,-0.4,1\\n-0.176471,0.336683,0.442623,-0.69697,-0.63357,-0.0342771,-0.842869,-0.466667,1\\n-0.176471,0.61809,0.409836,0,0,-0.0938897,-0.925705,-0.133333,0\\n-0.764706,0.0854271,0.311475,0,0,-0.195231,-0.845431,0.0333333,0\\n-0.411765,0.557789,0.377049,-0.111111,0.288416,0.153502,-0.538002,-0.566667,1\\n-0.882353,0.19598,0.409836,-0.212121,-0.479905,0.359165,-0.376601,-0.733333,0\\n-0.529412,-0.0351759,-0.0819672,-0.656566,-0.884161,-0.38003,-0.77626,-0.833333,1\\n-0.411765,0.0854271,0.180328,-0.131313,-0.822695,0.0760059,-0.842015,-0.6,1\\n0,-0.21608,0.442623,-0.414141,-0.905437,0.0998511,-0.695986,0,1\\n0,0.0753769,0.0163934,-0.393939,-0.825059,0.0909091,-0.420154,-0.866667,0\\n-0.764706,0.286432,0.278689,-0.252525,-0.56974,0.290611,-0.0213493,-0.666667,0\\n-0.882353,0.286432,-0.213115,-0.0909091,-0.541371,0.207154,-0.543126,-0.9,0\\n0,0.61809,-0.180328,0,0,-0.347243,-0.849701,0.466667,1\\n-0.294118,0.517588,0.0163934,-0.373737,-0.716312,0.0581222,-0.475662,-0.766667,1\\n-0.764706,0.467337,0.147541,-0.232323,-0.148936,-0.165425,-0.778822,-0.733333,0\\n0,0.266332,0.377049,-0.414141,-0.491726,-0.0849478,-0.622545,-0.9,1\\n0.647059,0.00502513,0.278689,-0.494949,-0.565012,0.0909091,-0.714774,-0.166667,0\\n-0.0588235,0.125628,0.180328,0,0,-0.296572,-0.349274,0.233333,1\\n0,0.678392,0,0,0,-0.0372578,-0.350128,-0.7,0\\n-0.764706,0.447236,-0.0491803,-0.333333,-0.680851,-0.0581222,-0.706234,-0.866667,0\\n-0.411765,-0.226131,0.344262,-0.171717,-0.900709,0.0670641,-0.93339,-0.533333,1\\n-0.411765,0.155779,0.606557,0,0,0.576751,-0.88813,-0.766667,0\\n-0.647059,0.507538,0.245902,0,0,-0.374069,-0.889838,-0.466667,1\\n-0.764706,0.20603,0.245902,-0.252525,-0.751773,0.183309,-0.883006,-0.733333,1\\n0.176471,0.61809,0.114754,-0.535354,-0.687943,-0.23994,-0.788215,-0.133333,0\\n0,0.376884,0.114754,-0.717172,-0.650118,-0.260805,-0.944492,0,1\\n0,0.286432,0.114754,-0.616162,-0.574468,-0.0909091,0.121264,-0.866667,0\\n-0.764706,0.246231,0.114754,-0.434343,-0.515366,-0.019374,-0.319385,-0.7,0\\n-0.294118,-0.19598,0.0819672,-0.393939,0,-0.219076,-0.799317,-0.333333,1\\n0,0.0653266,0.147541,-0.252525,-0.650118,0.174367,-0.549957,-0.966667,1\\n-0.764706,0.557789,0.213115,-0.656566,-0.77305,-0.207153,-0.69684,-0.8,0\\n-0.647059,0.135678,-0.180328,-0.79798,-0.799054,-0.120715,-0.532024,-0.866667,1\\n-0.176471,0.0954774,0.311475,-0.373737,0,0.0700448,-0.104184,-0.266667,0\\n-0.764706,0.125628,0.114754,-0.555556,-0.777778,0.0163934,-0.797609,-0.833333,1\\n-0.647059,-0.00502513,0.311475,-0.777778,-0.8487,-0.424739,-0.824082,-0.7,1\\n-0.647059,0.829146,0.213115,0,0,-0.0909091,-0.77199,-0.733333,0\\n-0.647059,0.155779,0.0819672,-0.212121,-0.669031,0.135618,-0.938514,-0.766667,1\\n-0.294118,0.949749,0.278689,0,0,-0.299553,-0.956447,0.266667,0\\n-0.529412,0.296482,-0.0163934,-0.757576,-0.453901,-0.180328,-0.616567,-0.666667,1\\n-0.647059,0.125628,0.213115,-0.393939,0,-0.0581222,-0.898377,-0.866667,0\\n0,0.246231,0.147541,-0.59596,0,-0.183308,-0.849701,-0.5,0\\n0.529412,0.527638,0.47541,-0.333333,-0.931442,-0.201192,-0.442357,-0.266667,0\\n-0.764706,0.125628,0.229508,-0.353535,0,0.0640835,-0.940222,0,1\\n-0.882353,0.577889,0.180328,-0.575758,-0.602837,-0.23696,-0.961571,-0.9,1\\n-0.882353,0.226131,0.0491803,-0.353535,-0.631206,0.0461997,-0.475662,-0.7,0\\n0.176471,0.798995,0.147541,0,0,0.0461997,-0.895816,-0.466667,1\\n-0.764706,0.0251256,0.409836,-0.272727,-0.716312,0.356185,-0.958155,-0.933333,0\\n-0.294118,0.0552764,0.147541,-0.353535,-0.839243,-0.0819672,-0.962425,-0.466667,1\\n-0.0588235,0.18593,0.180328,-0.616162,0,-0.311475,0.193851,-0.166667,1\\n-0.764706,-0.125628,-0.0491803,-0.676768,-0.877069,-0.0253353,-0.924851,-0.866667,1\\n-0.882353,0.809045,0,0,0,0.290611,-0.82579,-0.333333,0\\n0.411765,0.0653266,0.311475,0,0,-0.296572,-0.949616,-0.233333,1\\n-0.882353,-0.0452261,-0.0163934,-0.636364,-0.862884,-0.28763,-0.844577,-0.966667,1\\n0,0.658291,0.245902,-0.131313,-0.397163,0.42772,-0.845431,-0.833333,1\\n0,0.175879,0,0,0,0.00745157,-0.270709,-0.233333,1\\n-0.411765,0.155779,0.245902,0,0,-0.0700447,-0.773698,-0.233333,0\\n0.0588235,0.527638,0.278689,-0.313131,-0.595745,0.0193741,-0.304014,-0.6,0\\n-0.176471,0.788945,0.377049,0,0,0.18927,-0.783945,-0.333333,0\\n-0.882353,0.306533,0.147541,-0.737374,-0.751773,-0.228018,-0.663535,-0.966667,1\\n-0.882353,-0.0452261,0.213115,-0.575758,-0.827423,-0.228018,-0.491887,-0.5,1\\n-0.882353,0,0.114754,-0.292929,0,-0.0461997,-0.734415,-0.966667,1\\n-0.411765,0.226131,0.409836,0,0,0.0342773,-0.818958,-0.6,1\\n-0.0588235,-0.0452261,0.180328,0,0,0.0968703,-0.652434,0.2,1\\n-0.0588235,0.266332,0.442623,-0.272727,-0.744681,0.147541,-0.768574,-0.0666667,1\\n-0.882353,0.396985,-0.245902,-0.616162,-0.803783,-0.14456,-0.508113,-0.966667,1\\n-0.647059,0.165829,0,0,0,-0.299553,-0.906917,-0.933333,1\\n-0.647059,-0.00502513,0.0163934,-0.616162,-0.825059,-0.350224,-0.828352,-0.833333,1\\n-0.411765,0,0.311475,-0.353535,0,0.222057,-0.771136,-0.466667,0\\n-0.529412,-0.0753769,0.311475,0,0,0.257824,-0.864219,-0.733333,1\\n-0.529412,0.376884,0.377049,0,0,-0.0700447,-0.851409,-0.7,1\\n-0.647059,-0.386935,0.344262,-0.434343,0,0.0253354,-0.859095,-0.166667,1\\n-0.882353,-0.0954774,0.0163934,-0.757576,-0.898345,-0.18927,-0.571307,-0.9,1\\n-0.647059,-0.0954774,0.278689,0,0,0.272727,-0.58924,0,1\\n0.0588235,0.658291,0.442623,0,0,-0.0938897,-0.808711,-0.0666667,0\\n-0.882353,0.256281,-0.180328,-0.191919,-0.605201,-0.00745157,-0.24509,-0.766667,0\\n0.529412,0.296482,0,-0.393939,0,0.18927,-0.5807,-0.233333,0\\n0.411765,-0.115578,0.213115,-0.191919,-0.87234,0.052161,-0.743809,-0.1,1\\n-0.882353,0.969849,0.245902,-0.272727,-0.411348,0.0879285,-0.319385,-0.733333,0\\n-0.411765,0.899497,0.0491803,-0.333333,-0.231678,-0.0700447,-0.568745,-0.733333,0\\n-0.411765,0.58794,0.147541,0,0,-0.111773,-0.889838,0.4,1\\n-0.411765,0.0351759,0.770492,-0.252525,0,0.168405,-0.806149,0.466667,1\\n-0.529412,0.467337,0.278689,0,0,0.147541,-0.622545,0.533333,0\\n-0.529412,0.477387,0.213115,-0.494949,-0.307329,0.0402385,-0.737831,-0.7,1\\n-0.411765,-0.00502513,-0.114754,-0.434343,-0.803783,0.0134128,-0.640478,-0.7,1\\n-0.294118,0.246231,0.180328,0,0,-0.177347,-0.752348,-0.733333,0\\n0,0.0150754,0.0491803,-0.656566,0,-0.374069,-0.851409,0,1\\n-0.647059,-0.18593,0.409836,-0.676768,-0.843972,-0.180328,-0.805295,-0.966667,1\\n-0.882353,0.336683,0.672131,-0.434343,-0.669031,-0.0223547,-0.866781,-0.2,0\\n-0.647059,0.738693,0.344262,-0.030303,0.0992908,0.14456,0.758326,-0.866667,0\\n0,0.18593,0.0491803,-0.535354,-0.789598,0,0.411614,0,1\\n0,-0.155779,0.0491803,-0.555556,-0.843972,0.0670641,-0.601196,0,1\\n-0.764706,0.0552764,-0.0491803,-0.191919,-0.777778,0.0402385,-0.874466,-0.866667,1\\n-0.764706,0.226131,-0.147541,-0.131313,-0.626478,0.0789866,-0.369769,-0.766667,1\\n0.411765,0.407035,0.344262,-0.131313,-0.231678,0.168405,-0.615713,0.233333,0\\n0,-0.0150754,0.344262,-0.69697,-0.801418,-0.248882,-0.811272,-0.966667,1\\n-0.882353,-0.125628,-0.0163934,-0.252525,-0.822695,0.108793,-0.631939,-0.966667,1\\n-0.529412,0.567839,0.229508,0,0,0.439642,-0.863365,-0.633333,0\\n0,-0.0653266,0.639344,-0.212121,-0.829787,0.293592,-0.194705,-0.533333,1\\n-0.882353,0.0753769,0.180328,-0.393939,-0.806147,-0.0819672,-0.3655,-0.9,1\\n0,0.0552764,0.114754,-0.555556,0,-0.403875,-0.865073,-0.966667,1\\n-0.882353,0.0954774,-0.0163934,-0.838384,-0.56974,-0.242921,-0.257899,0,1\\n-0.882353,-0.0954774,0.0163934,-0.636364,-0.86052,-0.251863,0.0162254,-0.866667,1\\n-0.882353,0.256281,0.147541,-0.515152,-0.739953,-0.275708,-0.877882,-0.866667,1\\n-0.882353,0.19598,-0.114754,-0.737374,-0.881797,-0.33532,-0.891546,-0.9,1\\n-0.411765,0.165829,0.213115,-0.414141,0,-0.0372578,-0.502989,-0.533333,0\\n-0.0588235,0.0552764,0.639344,-0.272727,0,0.290611,-0.862511,-0.2,0\\n-0.411765,0.447236,0.344262,-0.474747,-0.326241,-0.0461997,-0.680615,0.233333,0\\n-0.647059,0.00502513,0.114754,-0.535354,-0.808511,-0.0581222,-0.256191,-0.766667,1\\n-0.882353,0.00502513,0.0819672,-0.414141,-0.536643,-0.0461997,-0.687447,-0.3,1\\n-0.411765,0.668342,0.245902,0,0,0.362146,-0.77626,-0.8,0\\n-0.882353,0.316583,0.0491803,-0.717172,-0.0189125,-0.293592,-0.734415,0,1\\n-0.529412,0.165829,0.180328,-0.757576,-0.794326,-0.341282,-0.671221,-0.466667,1\\n-0.529412,0.58794,0.278689,0,0,-0.019374,-0.380871,-0.666667,0\\n-0.764706,0.276382,-0.0491803,-0.515152,-0.349882,-0.174367,0.299744,-0.866667,1\\n-0.647059,-0.0351759,-0.0819672,-0.313131,-0.728132,-0.263785,-0.260461,-0.4,1\\n0,0.316583,0.0819672,-0.191919,0,0.0223547,-0.899231,-0.966667,0\\n-0.647059,-0.175879,0.147541,0,0,-0.371088,-0.734415,-0.866667,1\\n-0.647059,0.939698,0.147541,-0.373737,0,0.0402385,-0.860803,-0.866667,0\\n-0.529412,-0.0452261,0.0491803,0,0,-0.0461997,-0.92912,-0.666667,0\\n-0.411765,0.366834,0.377049,-0.171717,-0.791962,0.0432191,-0.822374,-0.533333,0\\n0.0588235,-0.276382,0.278689,-0.494949,0,-0.0581222,-0.827498,-0.433333,1\\n-0.411765,0.688442,0.0491803,0,0,-0.019374,-0.951324,-0.333333,0\\n-0.764706,0.236181,-0.213115,-0.353535,-0.609929,0.254843,-0.622545,-0.833333,1\\n-0.529412,0.155779,0.180328,0,0,-0.138599,-0.745517,-0.166667,0\\n0,0.0150754,0.0163934,0,0,-0.347243,-0.779675,-0.866667,1\\n-0.0588235,0.979899,0.213115,0,0,-0.228018,-0.0495303,-0.4,0\\n-0.882353,0.728643,0.114754,-0.010101,0.368794,0.263785,-0.467122,-0.766667,0\\n-0.294118,0.0251256,0.47541,-0.212121,0,0.0640835,-0.491033,-0.766667,1\\n-0.882353,0.125628,0.180328,-0.393939,-0.583924,0.0253354,-0.615713,-0.866667,1\\n-0.882353,0.437186,0.377049,-0.535354,-0.267139,0.263785,-0.147737,-0.966667,1\\n-0.882353,0.437186,0.213115,-0.555556,-0.855792,-0.219076,-0.847993,0,1\\n0,0.386935,-0.0163934,-0.292929,-0.605201,0.0312965,-0.610589,0,0\\n-0.647059,0.738693,0.377049,-0.333333,0.120567,0.0640835,-0.846285,-0.966667,0\\n-0.882353,-0.0251256,0.114754,-0.575758,0,-0.18927,-0.131512,-0.966667,1\\n-0.529412,0.447236,0.344262,-0.353535,0,0.147541,-0.59351,-0.466667,0\\n-0.882353,-0.165829,0.114754,0,0,-0.457526,-0.533732,-0.8,1\\n-0.647059,0.296482,0.0491803,-0.414141,-0.728132,-0.213115,-0.87959,-0.766667,0\\n-0.882353,0.19598,0.442623,-0.171717,-0.598109,0.350224,-0.633646,-0.833333,1\\n-0.764706,-0.0552764,0.114754,-0.636364,-0.820331,-0.225037,-0.587532,0,1\\n0,0.0251256,0.0491803,-0.0707071,-0.815603,0.210134,-0.64304,0,1\\n-0.764706,0.155779,0.0491803,-0.555556,0,-0.0819672,-0.707088,0,1\\n-0.0588235,0.517588,0.278689,-0.353535,-0.503546,0.278689,-0.625961,-0.5,0\\n-0.529412,0.849246,0.278689,-0.212121,-0.345154,0.102832,-0.841161,-0.666667,0\\n0,-0.0552764,0,0,0,0,-0.847993,-0.866667,1\\n-0.882353,0.819095,0.0491803,-0.393939,-0.574468,0.0163934,-0.786507,-0.433333,0\\n0,0.356784,0.540984,-0.0707071,-0.65721,0.210134,-0.824082,-0.833333,1\\n-0.882353,-0.0452261,0.344262,-0.494949,-0.574468,0.0432191,-0.867635,-0.266667,0\\n-0.764706,-0.00502513,0,0,0,-0.338301,-0.974381,-0.933333,1\\n-0.647059,-0.105528,0.213115,-0.676768,-0.799054,-0.0938897,-0.596072,-0.433333,1\\n-0.882353,-0.19598,0.213115,-0.777778,-0.858156,-0.105812,-0.616567,-0.966667,1\\n-0.764706,0.396985,0.229508,0,0,-0.23696,-0.923997,-0.733333,1\\n-0.882353,-0.0954774,0.114754,-0.838384,0,-0.269747,-0.0947908,-0.5,1\\n0,0.417085,0,0,0,0.263785,-0.891546,-0.733333,0\\n0.411765,0.407035,0.393443,-0.333333,0,0.114754,-0.858241,-0.333333,1\\n-0.411765,0.477387,0.229508,0,0,-0.108793,-0.695986,-0.766667,1\\n-0.882353,-0.0251256,0.147541,-0.69697,0,-0.457526,-0.941076,0,1\\n-0.294118,0.0753769,0.442623,0,0,0.0968703,-0.445773,-0.666667,1\\n0,0.899497,0.704918,-0.494949,0,0.0223547,-0.695132,-0.333333,0\\n-0.764706,-0.165829,0.0819672,-0.535354,-0.881797,-0.0402384,-0.642186,-0.966667,1\\n-0.529412,0.175879,0.0491803,-0.454545,-0.716312,-0.0104321,-0.870196,-0.9,1\\n-0.0588235,0.0854271,0.147541,0,0,-0.0909091,-0.251067,-0.6,0\\n-0.529412,0.175879,0.0163934,-0.757576,0,-0.114754,-0.742101,-0.7,0\\n0,0.809045,0.278689,0.272727,-0.966903,0.770492,1,-0.866667,0\\n-0.882353,0.00502513,0.180328,-0.757576,-0.834515,-0.245902,-0.504697,-0.766667,1\\n0,-0.0452261,0.311475,-0.0909091,-0.782506,0.0879285,-0.784799,-0.833333,1\\n0,0.0452261,0.0491803,-0.252525,-0.8487,0.00149028,-0.631085,-0.966667,0\\n0,0.20603,0.213115,-0.636364,-0.851064,-0.0909091,-0.823228,-0.833333,1\\n-0.882353,-0.175879,0.0491803,-0.737374,-0.775414,-0.368107,-0.712212,-0.933333,1\\n-0.764706,0.346734,0.147541,0,0,-0.138599,-0.603757,-0.933333,0\\n0,-0.0854271,0.114754,-0.353535,-0.503546,0.18927,-0.741247,-0.866667,1\\n-0.764706,0.19598,0,0,0,-0.415797,-0.356106,0.7,1\\n-0.764706,0.00502513,-0.114754,-0.434343,-0.751773,0.126677,-0.641332,-0.9,1\\n0.647059,0.758794,0.0163934,-0.393939,0,0.00149028,-0.885568,-0.433333,0\\n-0.882353,0.356784,-0.114754,0,0,-0.204173,-0.479932,0.366667,1\\n-0.411765,-0.135678,0.114754,-0.434343,-0.832151,-0.0998509,-0.755764,-0.9,1\\n0.0588235,0.346734,0.213115,-0.333333,-0.858156,-0.228018,-0.673783,1,1\\n0.0588235,0.20603,0.180328,-0.555556,-0.867612,-0.38003,-0.440649,-0.1,1\\n-0.882353,-0.286432,0.0163934,0,0,-0.350224,-0.711358,-0.833333,1\\n-0.0588235,-0.256281,0.147541,-0.191919,-0.884161,0.052161,-0.46456,-0.4,1\\n-0.411765,-0.115578,0.278689,-0.393939,0,-0.177347,-0.846285,-0.466667,1\\n0.176471,0.155779,0.606557,0,0,-0.28465,-0.193851,-0.566667,1\\n0,0.246231,-0.0819672,-0.737374,-0.751773,-0.350224,-0.680615,0,1\\n0,-0.256281,-0.147541,-0.79798,-0.914894,-0.171386,-0.836892,-0.966667,1\\n0,-0.0251256,0.0491803,-0.272727,-0.763593,0.0968703,-0.554227,-0.866667,1\\n-0.0588235,0.20603,0,0,0,-0.105812,-0.910333,-0.433333,0\\n-0.294118,0.547739,0.278689,-0.171717,-0.669031,0.374069,-0.578992,-0.8,1\\n-0.882353,0.447236,0.344262,-0.191919,0,0.230999,-0.548249,-0.766667,1\\n0,0.376884,0.147541,-0.232323,0,-0.0104321,-0.921435,-0.966667,1\\n0,0.19598,0.0819672,-0.454545,0,0.156483,-0.845431,-0.966667,1\\n-0.176471,0.366834,0.47541,0,0,-0.108793,-0.887276,-0.0333333,1\\n-0.529412,0.145729,0.0491803,0,0,-0.138599,-0.959009,-0.9,1\\n0,0.376884,0.377049,-0.454545,0,-0.186289,-0.869342,0.266667,1\\n-0.764706,0.0552764,0.311475,-0.0909091,-0.548463,0.004471,-0.459436,-0.733333,0\\n-0.176471,0.145729,0.245902,-0.656566,-0.739953,-0.290611,-0.668659,-0.666667,1\\n-0.0588235,0.266332,0.213115,-0.232323,-0.822695,-0.228018,-0.928266,-0.4,1\\n-0.529412,0.326633,0.409836,-0.373737,0,-0.165425,-0.708796,0.4,1\\n-0.647059,0.58794,0.147541,-0.393939,-0.224586,0.0581222,-0.772844,-0.533333,0\\n0,0.236181,0.442623,-0.252525,0,0.0491804,-0.898377,-0.733333,1\\n-0.529412,-0.145729,-0.0491803,-0.555556,-0.884161,-0.171386,-0.805295,-0.766667,1\\n0,-0.155779,0.344262,-0.373737,-0.704492,0.138599,-0.867635,-0.933333,1\\n0,0.457286,0,0,0,0.317437,-0.528608,-0.666667,0\\n0,0.356784,0.114754,-0.151515,-0.408983,0.260805,-0.75491,-0.9,0\\n-0.882353,0.396985,0.0163934,-0.171717,0.134752,0.213115,-0.608881,0,1\\n0,0.738693,0.278689,-0.353535,-0.373522,0.385991,-0.0768574,0.233333,1\\n-0.529412,-0.00502513,0.180328,-0.656566,0,-0.23696,-0.815542,-0.766667,1\\n-0.0588235,0.949749,0.311475,0,0,-0.222057,-0.596072,0.533333,1\\n-0.764706,-0.165829,0.0655738,-0.434343,-0.843972,0.0968703,-0.529462,-0.9,1\\n-0.764706,-0.105528,0.47541,-0.393939,0,-0.00149028,-0.81725,-0.3,1\\n-0.529412,-0.00502513,0.114754,-0.232323,0,-0.0223547,-0.942784,-0.6,1\\n-0.529412,0.256281,0.147541,-0.636364,-0.711584,-0.138599,-0.089667,-0.2,0\\n-0.647059,-0.19598,0,0,0,0,-0.918019,-0.966667,1\\n-0.294118,0.668342,0.213115,0,0,-0.207153,-0.807003,0.5,1\\n-0.411765,0.105528,0.114754,0,0,-0.225037,-0.81725,-0.7,1\\n-0.764706,-0.18593,0.180328,-0.69697,-0.820331,-0.102832,-0.599488,-0.866667,1\\n-0.176471,0.959799,0.147541,-0.333333,-0.65721,-0.251863,-0.927412,0.133333,0\\n-0.294118,0.547739,0.213115,-0.353535,-0.543735,-0.126677,-0.350128,-0.4,1\\n-0.764706,0.175879,0.47541,-0.616162,-0.832151,-0.248882,-0.799317,0,1\\n-0.647059,-0.155779,0.180328,-0.353535,0,0.108793,-0.838599,-0.766667,1\\n-0.294118,0,0.114754,-0.171717,0,0.162444,-0.445773,-0.333333,0\\n-0.176471,-0.0552764,0.0491803,-0.494949,-0.813239,-0.00745157,-0.436379,-0.333333,1\\n-0.647059,-0.0351759,0.278689,-0.212121,0,0.111773,-0.863365,-0.366667,1\\n0.176471,-0.246231,0.344262,0,0,-0.00745157,-0.842015,-0.433333,1\\n0,0.809045,0.47541,-0.474747,-0.787234,0.0879285,-0.798463,-0.533333,0\\n-0.882353,0.306533,-0.0163934,-0.535354,-0.598109,-0.147541,-0.475662,0,1\\n-0.764706,-0.155779,-0.180328,-0.535354,-0.820331,-0.0938897,-0.239966,0,1\\n-0.0588235,0.20603,0.278689,0,0,-0.254843,-0.717336,0.433333,1\\n0.411765,-0.155779,0.180328,-0.373737,0,-0.114754,-0.81298,-0.166667,0\\n0,0.396985,0.0163934,-0.656566,-0.503546,-0.341282,-0.889838,0,1\\n0.0588235,-0.0854271,0.114754,0,0,-0.278688,-0.895816,0.233333,1\\n-0.764706,-0.0854271,0.0163934,0,0,-0.186289,-0.618275,-0.966667,1\\n-0.647059,-0.00502513,-0.114754,-0.616162,-0.79669,-0.23696,-0.935098,-0.9,1\\n-0.647059,0.638191,0.147541,-0.636364,-0.751773,-0.0581222,-0.837746,-0.766667,0\\n0.0588235,0.457286,0.442623,-0.313131,-0.609929,-0.0968703,-0.408198,0.0666667,0\\n0.529412,-0.236181,-0.0163934,0,0,-0.0223547,-0.912895,-0.333333,1\\n-0.294118,0.296482,0.47541,-0.858586,-0.229314,-0.415797,-0.569599,0.3,1\\n-0.764706,-0.316583,0.147541,-0.353535,-0.843972,-0.254843,-0.906917,-0.866667,1\\n-0.647059,0.246231,0.311475,-0.333333,-0.692671,-0.0104321,-0.806149,-0.833333,1\\n-0.294118,0.145729,0,0,0,0,-0.905209,-0.833333,1\\n0.0588235,0.306533,0.147541,0,0,0.0193741,-0.509821,-0.2,0\\n-0.647059,0.256281,-0.0491803,0,0,-0.0581222,-0.93766,-0.9,1\\n-0.647059,-0.125628,-0.0163934,-0.636364,0,-0.350224,-0.687447,0,1\\n-0.882353,-0.0251256,0.0491803,-0.616162,-0.806147,-0.457526,-0.811272,0,1\\n-0.647059,0.165829,0.213115,-0.69697,-0.751773,-0.216095,-0.975235,-0.9,1\\n0,0.175879,0.0819672,-0.373737,-0.555556,-0.0819672,-0.645602,-0.966667,1\\n0,0.115578,0.0655738,0,0,-0.266766,-0.502989,-0.666667,1\\n-0.764706,0.226131,-0.0163934,-0.636364,-0.749409,-0.111773,-0.454313,-0.966667,1\\n0,0.0753769,0.245902,0,0,0.350224,-0.480786,-0.9,1\\n-0.882353,-0.135678,0.0819672,0.0505051,-0.846336,0.230999,-0.283518,-0.733333,1\\n-0.294118,-0.0854271,0,0,0,-0.111773,-0.63877,-0.666667,1\\n-0.882353,-0.226131,-0.0819672,-0.393939,-0.867612,-0.00745157,0.00170794,-0.9,1\\n-0.529412,0.326633,0,0,0,-0.019374,-0.808711,-0.933333,0\\n0,0.0552764,0.47541,0,0,-0.117735,-0.898377,-0.166667,1\\n0,-0.427136,-0.0163934,0,0,-0.353204,-0.438941,0.533333,1\\n0,0.276382,0.311475,-0.252525,-0.503546,0.0819672,-0.380017,-0.933333,1\\n-0.647059,0.296482,0.508197,-0.010101,-0.63357,0.0849479,-0.239966,-0.633333,0\\n-0.0588235,0.00502513,0.213115,-0.191919,-0.491726,0.174367,-0.502135,-0.266667,0\\n-0.647059,0.286432,0.180328,-0.494949,-0.550827,-0.0342771,-0.59778,-0.8,0\\n0.176471,-0.0954774,0.393443,-0.353535,0,0.0402385,-0.362084,0.166667,0\\n-0.529412,-0.155779,0.47541,-0.535354,-0.867612,0.177347,-0.930828,-0.866667,1\\n-0.882353,-0.115578,0.278689,-0.414141,-0.820331,-0.0461997,-0.75491,-0.733333,1\\n-0.0588235,0.869347,0.47541,-0.292929,-0.468085,0.028316,-0.70538,-0.466667,0\\n-0.411765,0.879397,0.245902,-0.454545,-0.510638,0.299553,-0.183604,0.0666667,0\\n-0.529412,0.316583,0.114754,-0.575758,-0.607565,-0.0134128,-0.929974,-0.766667,1\\n-0.882353,0.648241,0.344262,-0.131313,-0.841608,-0.0223547,-0.775406,-0.0333333,1\\n-0.529412,0.899497,0.803279,-0.373737,0,-0.150522,-0.485909,-0.466667,1\\n-0.882353,0.165829,0.147541,-0.434343,0,-0.183308,-0.8924,0,1\\n-0.647059,-0.155779,0.114754,-0.393939,-0.749409,-0.0491803,-0.561913,-0.866667,1\\n-0.294118,0.145729,0.442623,0,0,-0.171386,-0.855679,0.5,1\\n-0.882353,-0.115578,0.0163934,-0.515152,-0.895981,-0.108793,-0.706234,-0.933333,1\\n-0.882353,-0.155779,0.0491803,-0.535354,-0.728132,0.0998511,-0.664389,-0.766667,1\\n-0.176471,0.246231,0.147541,-0.333333,-0.491726,-0.23994,-0.92912,-0.466667,1\\n-0.882353,-0.0251256,0.147541,-0.191919,0,0.135618,-0.880444,-0.7,1\\n-0.0588235,0.105528,0.245902,0,0,-0.171386,-0.864219,0.233333,1\\n0.294118,0.0351759,0.114754,-0.191919,0,0.377049,-0.959009,-0.3,1\\n0.294118,-0.145729,0.213115,0,0,-0.102832,-0.810418,-0.533333,1\\n-0.294118,0.256281,0.245902,0,0,0.00745157,-0.963279,0.1,0\\n0,0.98995,0.0819672,-0.353535,-0.352246,0.230999,-0.637916,-0.766667,0\\n-0.882353,-0.125628,0.114754,-0.313131,-0.817967,0.120715,-0.724167,-0.9,1\\n-0.294118,-0.00502513,-0.0163934,-0.616162,-0.87234,-0.198212,-0.642186,-0.633333,1\\n0,-0.0854271,0.311475,0,0,-0.0342771,-0.553373,-0.8,1\\n-0.764706,-0.0452261,-0.114754,-0.717172,-0.791962,-0.222057,-0.427839,-0.966667,1\\n-0.882353,-0.00502513,0.180328,-0.393939,-0.957447,0.150522,-0.714774,0,1\\n-0.294118,-0.0753769,0.0163934,-0.353535,-0.702128,-0.0461997,-0.994022,-0.166667,1\\n-0.529412,0.547739,0.180328,-0.414141,-0.702128,-0.0670641,-0.777968,-0.466667,1\\n0,0.21608,0.0819672,-0.393939,-0.609929,0.0223547,-0.893254,-0.6,0\\n-0.647059,-0.21608,0.147541,0,0,-0.0312965,-0.836038,-0.4,1\\n-0.764706,0.306533,0.57377,0,0,-0.326379,-0.837746,0,1\\n-0.647059,0.115578,-0.0491803,-0.373737,-0.895981,-0.120715,-0.699402,-0.966667,1\\n-0.764706,-0.0150754,-0.0163934,-0.656566,-0.716312,0.0342773,-0.897523,-0.966667,1\\n-0.882353,0.437186,0.409836,-0.393939,-0.219858,-0.102832,-0.304868,-0.933333,1\\n-0.882353,0.19598,-0.278689,-0.0505051,-0.851064,0.0581222,-0.827498,-0.866667,1\\n-0.294118,0.0854271,-0.278689,-0.59596,-0.692671,-0.28465,-0.372331,-0.533333,1\\n-0.764706,0.18593,0.311475,0,0,0.278689,-0.474808,0,0\\n0.176471,0.336683,0.114754,0,0,-0.195231,-0.857387,-0.5,1\\n-0.764706,0.979899,0.147541,1,0,0.0342773,-0.575576,0.366667,0\\n0,0.517588,0.47541,-0.0707071,0,0.254843,-0.749787,0,0\\n-0.294118,0.0954774,-0.0163934,-0.454545,0,-0.254843,-0.890692,-0.8,1\\n0.411765,0.21608,0.278689,-0.656566,0,-0.210134,-0.845431,0.366667,1\\n-0.0588235,0.00502513,0.245902,0,0,0.153502,-0.904355,-0.3,1\\n-0.0588235,0.246231,0.245902,-0.515152,0.41844,-0.14456,-0.479932,0.0333333,0\\n-0.882353,-0.0653266,-0.0819672,-0.777778,0,-0.329359,-0.710504,-0.966667,1\\n-0.0588235,0.437186,0.0819672,0,0,0.0402385,-0.956447,-0.333333,0\\n-0.294118,0.0351759,0.0819672,0,0,-0.275708,-0.853971,-0.733333,1\\n-0.647059,0.768844,0.409836,-0.454545,-0.631206,-0.00745157,-0.0811272,0.0333333,0\\n0,-0.266332,0,0,0,-0.371088,-0.774552,-0.866667,1\\n0.294118,0.115578,0.377049,-0.191919,0,0.394933,-0.276687,-0.2,0\\n-0.764706,0.125628,0.278689,0.010101,-0.669031,0.174367,-0.917165,-0.9,1\\n-0.647059,0.326633,0.311475,0,0,0.0253354,-0.723313,-0.233333,0\\n-0.764706,-0.175879,-0.147541,-0.555556,-0.728132,-0.150522,0.384287,-0.866667,1\\n-0.294118,0.236181,0.180328,-0.0909091,-0.456265,0.00149028,-0.440649,-0.566667,1\\n0,0.889447,0.344262,-0.717172,-0.562648,-0.0461997,-0.484202,-0.966667,0\\n0,-0.326633,0.245902,0,0,0.350224,-0.900939,-0.166667,1\\n-0.882353,-0.105528,-0.606557,-0.616162,-0.940898,-0.171386,-0.58924,0,1\\n-0.882353,0.738693,0.213115,0,0,0.0968703,-0.99146,-0.433333,0\\n-0.882353,0.0954774,-0.377049,-0.636364,-0.716312,-0.311475,-0.719044,-0.833333,1\\n-0.882353,0.0854271,0.442623,-0.616162,0,-0.19225,-0.725021,-0.9,1\\n-0.294118,-0.0351759,0,0,0,-0.293592,-0.904355,-0.766667,1\\n-0.882353,0.246231,0.213115,-0.272727,0,-0.171386,-0.981213,-0.7,1\\n-0.176471,0.507538,0.278689,-0.414141,-0.702128,0.0491804,-0.475662,0.1,0\\n-0.529412,0.839196,0,0,0,-0.153502,-0.885568,-0.5,0\\n-0.882353,0.246231,-0.0163934,-0.353535,0,0.0670641,-0.627669,0,1\\n-0.882353,0.819095,0.278689,-0.151515,-0.307329,0.19225,0.00768574,-0.966667,0\\n-0.882353,-0.0753769,0.0163934,-0.494949,-0.903073,-0.418778,-0.654996,-0.866667,1\\n0,0.527638,0.344262,-0.212121,-0.356974,0.23696,-0.836038,-0.8,1\\n-0.882353,0.115578,0.0163934,-0.737374,-0.56974,-0.28465,-0.948762,-0.933333,1\\n-0.647059,0.0653266,-0.114754,-0.575758,-0.626478,-0.0789866,-0.81725,-0.9,1\\n-0.647059,0.748744,-0.0491803,-0.555556,-0.541371,-0.019374,-0.560205,-0.5,0\\n-0.176471,0.688442,0.442623,-0.151515,-0.241135,0.138599,-0.394535,-0.366667,0\\n-0.294118,0.0552764,0.311475,-0.434343,0,-0.0312965,-0.316823,-0.833333,1\\n0.294118,0.386935,0.213115,-0.474747,-0.659574,0.0760059,-0.590948,-0.0333333,0\\n-0.647059,0.0653266,0.180328,0,0,-0.230999,-0.889838,-0.8,1\\n-0.294118,0.175879,0.57377,0,0,-0.14456,-0.932536,-0.7,1\\n-0.764706,-0.316583,0.0163934,-0.737374,-0.964539,-0.400894,-0.847139,-0.933333,1\\n0.0588235,0.125628,0.344262,-0.515152,0,-0.159463,0.028181,-0.0333333,0\\n0,0.19598,0,0,0,-0.0342771,-0.9462,-0.9,0\\n-0.764706,0.125628,0.409836,-0.151515,-0.621749,0.14456,-0.856533,-0.766667,1\\n-0.764706,-0.0753769,0.245902,-0.59596,0,-0.278688,0.383433,-0.766667,1\\n-0.294118,0.839196,0.540984,0,0,0.216095,0.181042,-0.2,1\\n0,-0.0552764,0.147541,-0.454545,-0.728132,0.296572,-0.770282,0,1\\n-0.764706,0.0854271,0.0491803,0,0,-0.0819672,-0.931682,0,1\\n-0.529412,-0.0954774,0.442623,-0.0505051,-0.87234,0.123696,-0.757472,-0.733333,1\\n0,0.256281,0.114754,0,0,-0.263785,-0.890692,0,1\\n0,0.326633,0.278689,0,0,-0.0342771,-0.730999,0,1\\n-0.411765,0.286432,0.311475,0,0,0.0312965,-0.943638,-0.2,1\\n-0.529412,-0.0552764,0.0655738,-0.555556,0,-0.263785,-0.940222,0,1\\n-0.176471,0.145729,0.0491803,0,0,-0.183308,-0.441503,-0.566667,0\\n0,0.0251256,0.278689,-0.191919,-0.787234,0.028316,-0.863365,-0.9,1\\n-0.764706,0.115578,-0.0163934,0,0,-0.219076,-0.773698,-0.933333,1\\n-0.882353,0.286432,0.344262,-0.656566,-0.567376,-0.180328,-0.968403,-0.966667,1\\n0.176471,-0.0753769,0.0163934,0,0,-0.228018,-0.923997,-0.666667,1\\n0.529412,0.0452261,0.180328,0,0,-0.0700447,-0.669513,-0.433333,0\\n-0.411765,0.0452261,0.213115,0,0,-0.14158,-0.935952,-0.1,1\\n-0.764706,-0.0552764,0.245902,-0.636364,-0.843972,-0.0581222,-0.512383,-0.933333,1\\n-0.176471,-0.0251256,0.245902,-0.353535,-0.78487,0.219076,-0.322801,-0.633333,0\\n-0.882353,0.00502513,0.213115,-0.757576,-0.891253,-0.418778,-0.939368,-0.766667,1\\n0,0.0251256,0.409836,-0.656566,-0.751773,-0.126677,-0.4731,-0.8,1\\n-0.529412,0.286432,0.147541,0,0,0.0223547,-0.807857,-0.9,1\\n-0.294118,0.477387,0.311475,0,0,-0.120715,-0.914603,-0.0333333,0\\n-0.529412,-0.0954774,0,0,0,-0.165425,-0.545687,-0.666667,1\\n-0.647059,0.0351759,0.180328,-0.393939,-0.640662,-0.177347,-0.443211,-0.8,1\\n-0.764706,0.577889,0.213115,-0.292929,0.0401891,0.174367,-0.952178,-0.7,1\\n-0.882353,0.678392,0.213115,-0.656566,-0.659574,-0.302534,-0.684885,-0.6,0\\n0,0.798995,-0.180328,-0.272727,-0.624113,0.126677,-0.678053,-0.966667,0\\n0.294118,0.366834,0.377049,-0.292929,-0.692671,-0.156483,-0.844577,-0.3,0\\n0,0.0753769,-0.0163934,-0.494949,0,-0.213115,-0.953032,-0.933333,1\\n-0.882353,-0.0854271,-0.114754,-0.494949,-0.763593,-0.248882,-0.866781,-0.933333,1\\n-0.882353,0.175879,-0.0163934,-0.535354,-0.749409,0.00745157,-0.668659,-0.8,1\\n-0.411765,0.236181,0.213115,-0.191919,-0.817967,0.0163934,-0.836892,-0.766667,1\\n-0.764706,0.20603,-0.114754,0,0,-0.201192,-0.678053,-0.8,1\\n-0.882353,0.0653266,0.147541,-0.434343,-0.680851,0.0193741,-0.945346,-0.966667,1\\n-0.764706,0.557789,-0.147541,-0.454545,0.276596,0.153502,-0.861657,-0.866667,0\\n-0.764706,0.0150754,-0.0491803,-0.292929,-0.787234,-0.350224,-0.934244,-0.966667,1\\n-0.882353,0.20603,0.311475,-0.030303,-0.527187,0.159464,-0.0742955,-0.333333,1\\n-0.647059,-0.19598,0.344262,-0.373737,-0.834515,0.0193741,0.0367208,-0.8,0\\n0.176471,0.628141,0.377049,0,0,-0.174367,-0.911187,0.1,1\\n-0.882353,1,0.245902,-0.131313,0,0.278689,0.123826,-0.966667,0\\n-0.0588235,0.678392,0.737705,-0.0707071,-0.453901,0.120715,-0.925705,-0.266667,0\\n0.0588235,0.457286,0.311475,-0.0707071,-0.692671,0.129657,-0.52263,-0.366667,0\\n-0.294118,0.155779,-0.0163934,-0.212121,0,0.004471,-0.857387,-0.366667,0\\n-0.882353,0.125628,0.311475,-0.0909091,-0.687943,0.0372578,-0.881298,-0.9,1\\n-0.529412,0.457286,0.344262,-0.636364,0,-0.0312965,-0.865927,0.633333,0\\n0.176471,0.115578,0.147541,-0.454545,0,-0.180328,-0.9462,-0.366667,0\\n-0.294118,-0.0150754,-0.0491803,-0.333333,-0.550827,0.0134128,-0.699402,-0.266667,1\\n0.0588235,0.547739,0.278689,-0.393939,-0.763593,-0.0789866,-0.926558,-0.2,1\\n-0.294118,0.658291,0.114754,-0.474747,-0.602837,0.00149028,-0.527754,-0.0666667,1\\n-0.882353,-0.00502513,-0.0491803,-0.79798,0,-0.242921,-0.596072,0,1\\n0.176471,-0.316583,0.737705,-0.535354,-0.884161,0.0581222,-0.823228,-0.133333,1\\n-0.647059,0.236181,0.639344,-0.292929,-0.432624,0.707899,-0.315115,-0.966667,1\\n-0.0588235,-0.0854271,0.344262,0,0,0.0611028,-0.565329,0.566667,1\\n-0.294118,0.959799,0.147541,0,0,-0.0789866,-0.786507,-0.666667,0\\n0.0588235,0.567839,0.409836,0,0,-0.260805,-0.870196,0.0666667,0\\n0,-0.0653266,-0.0163934,0,0,0.052161,-0.842015,-0.866667,1\\n-0.647059,0.21608,-0.147541,0,0,0.0730254,-0.958155,-0.866667,0\\n-0.764706,0.0150754,-0.0491803,-0.656566,-0.373522,-0.278688,-0.542272,-0.933333,1\\n-0.764706,-0.437186,-0.0819672,-0.434343,-0.893617,-0.278688,-0.783091,-0.966667,1\\n0,0.628141,0.245902,-0.272727,0,0.47839,-0.755764,-0.833333,0\\n0,-0.0452261,0.0491803,-0.212121,-0.751773,0.329359,-0.754056,-0.966667,1\\n-0.529412,0.256281,0.311475,0,0,-0.0372578,-0.608881,-0.8,0\\n-0.411765,0.366834,0.344262,0,0,0,-0.520068,0.6,1\\n-0.764706,0.296482,0.213115,-0.474747,-0.515366,-0.0104321,-0.561913,-0.866667,1\\n-0.647059,0.306533,0.0491803,0,0,-0.311475,-0.798463,-0.966667,1\\n-0.882353,0.0753769,-0.180328,-0.616162,0,-0.156483,-0.912041,-0.733333,1\\n-0.882353,0.407035,0.213115,-0.474747,-0.574468,-0.281669,-0.359522,-0.933333,1\\n-0.882353,0.447236,0.344262,-0.0707071,-0.574468,0.374069,-0.780529,-0.166667,0\\n-0.0588235,0.0753769,0.311475,0,0,-0.266766,-0.335611,-0.566667,1\\n0.529412,0.58794,0.868852,0,0,0.260805,-0.847139,-0.233333,0\\n-0.764706,0.21608,0.147541,-0.353535,-0.775414,0.165425,-0.309991,-0.933333,1\\n-0.176471,0.296482,0.114754,-0.010101,-0.704492,0.147541,-0.691716,-0.266667,0\\n-0.764706,-0.0954774,-0.0163934,0,0,-0.299553,-0.903501,-0.866667,1\\n-0.176471,0.427136,0.47541,-0.515152,0.134752,-0.0938897,-0.957301,-0.266667,0\\n-0.647059,0.698492,0.213115,-0.616162,-0.704492,-0.108793,-0.837746,-0.666667,0\\n0,-0.00502513,0,0,0,-0.254843,-0.850555,-0.966667,1\\n-0.529412,0.276382,0.442623,-0.777778,-0.63357,0.028316,-0.555935,-0.766667,1\\n-0.529412,0.18593,0.147541,0,0,0.326379,-0.29462,-0.833333,1\\n-0.764706,0.226131,0.245902,-0.454545,-0.527187,0.0700448,-0.654142,-0.833333,1\\n-0.294118,0.256281,0.278689,-0.373737,0,-0.177347,-0.584116,-0.0666667,0\\n-0.882353,0.688442,0.442623,-0.414141,0,0.0432191,-0.293766,0.0333333,0\\n-0.764706,0.296482,0,0,0,0.147541,-0.807003,-0.333333,1\\n-0.529412,0.105528,0.245902,-0.59596,-0.763593,-0.153502,-0.965841,-0.8,1\\n-0.294118,-0.19598,0.311475,-0.272727,0,0.186289,-0.915457,-0.766667,1\\n0.176471,0.155779,0,0,0,0,-0.843723,-0.7,0\\n-0.764706,0.276382,-0.245902,-0.575758,-0.208038,0.0253354,-0.916311,-0.966667,1\\n0.0588235,0.648241,0.278689,0,0,-0.0223547,-0.940222,-0.2,0\\n-0.764706,-0.0653266,0.0491803,-0.353535,-0.621749,0.132638,-0.491033,-0.933333,0\\n-0.647059,0.58794,0.0491803,-0.737374,-0.0851064,-0.0700447,-0.814688,-0.9,1\\n-0.411765,0.266332,0.278689,-0.454545,-0.947991,-0.117735,-0.691716,-0.366667,1\\n0.176471,0.296482,0.0163934,-0.272727,0,0.228018,-0.690009,-0.433333,0\\n0,0.346734,-0.0491803,-0.59596,-0.312057,-0.213115,-0.766012,0,1\\n-0.647059,0.0251256,0.213115,0,0,-0.120715,-0.963279,-0.633333,1\\n-0.176471,0.879397,-0.180328,-0.333333,-0.0732861,0.0104323,-0.36123,-0.566667,0\\n-0.647059,0.738693,0.278689,-0.212121,-0.562648,0.00745157,-0.238258,-0.666667,0\\n0.176471,-0.0552764,0.180328,-0.636364,0,-0.311475,-0.558497,0.166667,1\\n-0.882353,0.0854271,-0.0163934,-0.0707071,-0.579196,0.0581222,-0.712212,-0.9,1\\n-0.411765,-0.0251256,0.245902,-0.454545,0,0.0611028,-0.743809,0.0333333,0\\n-0.529412,-0.165829,0.409836,-0.616162,0,-0.126677,-0.795901,-0.566667,1\\n-0.882353,0.145729,0.0819672,-0.272727,-0.527187,0.135618,-0.819812,0,1\\n-0.882353,0.497487,0.114754,-0.414141,-0.699764,-0.126677,-0.768574,-0.3,0\\n-0.411765,0.175879,0.409836,-0.393939,-0.751773,0.165425,-0.852263,-0.3,1\\n-0.882353,0.115578,0.540984,0,0,-0.0223547,-0.840307,-0.2,1\\n-0.529412,0.125628,0.278689,-0.191919,0,0.174367,-0.865073,-0.433333,1\\n-0.882353,0.165829,0.278689,-0.414141,-0.574468,0.0760059,-0.64304,-0.866667,1\\n0,0.417085,0.377049,-0.474747,0,-0.0342771,-0.69684,-0.966667,1\\n-0.764706,0.758794,0.442623,0,0,-0.317437,-0.788215,-0.966667,1\\n-0.764706,-0.0753769,-0.147541,0,0,-0.102832,-0.9462,-0.966667,1\\n-0.647059,0.306533,0.278689,-0.535354,-0.813239,-0.153502,-0.790777,-0.566667,0\\n-0.0588235,0.20603,0.409836,0,0,-0.153502,-0.845431,-0.966667,0\\n-0.764706,0.748744,0.442623,-0.252525,-0.716312,0.326379,-0.514944,-0.9,0\\n-0.764706,0.0653266,-0.0819672,-0.454545,-0.609929,-0.135618,-0.702818,-0.966667,1\\n-0.764706,0.0552764,0.229508,0,0,-0.305514,-0.588386,0.0666667,1\\n-0.529412,-0.0452261,-0.0163934,-0.353535,0,0.0551417,-0.824082,-0.766667,1\\n0,0.266332,0.409836,-0.454545,-0.716312,-0.183308,-0.626815,0,1\\n-0.0588235,-0.346734,0.180328,-0.535354,0,-0.0461997,-0.554227,-0.3,1\\n-0.764706,-0.00502513,-0.0163934,-0.656566,-0.621749,0.0909091,-0.679761,0,1\\n-0.882353,0.0251256,0.213115,0,0,0.177347,-0.816396,-0.3,0\\n0.294118,0.20603,0.311475,-0.252525,-0.64539,0.260805,-0.396243,-0.1,0\\n-0.647059,0.0251256,-0.278689,-0.59596,-0.777778,-0.0819672,-0.725021,-0.833333,1\\n-0.882353,0.0954774,-0.0491803,-0.636364,-0.725768,-0.150522,-0.87959,-0.966667,1\\n0.0588235,0.407035,0.540984,0,0,-0.0253353,-0.439795,-0.2,0\\n0.529412,0.537688,0.442623,-0.252525,-0.669031,0.210134,-0.0640478,-0.4,1\\n0.411765,0.00502513,0.377049,-0.333333,-0.751773,-0.105812,-0.649872,-0.166667,1\\n-0.882353,0.477387,0.540984,-0.171717,0,0.469449,-0.760888,-0.8,0\\n-0.882353,-0.18593,0.213115,-0.171717,-0.865248,0.38003,-0.130658,-0.633333,1\\n-0.647059,0.879397,0.147541,-0.555556,-0.527187,0.0849479,-0.71819,-0.5,0\\n-0.294118,0.628141,0.0163934,0,0,-0.275708,-0.914603,-0.0333333,0\\n-0.529412,0.366834,0.147541,0,0,-0.0700447,-0.0572161,-0.966667,0\\n-0.882353,0.21608,0.278689,-0.212121,-0.825059,0.162444,-0.843723,-0.766667,1\\n-0.647059,0.0854271,0.0163934,-0.515152,0,-0.225037,-0.876174,-0.866667,1\\n0,0.819095,0.442623,-0.111111,0.205674,0.290611,-0.877028,-0.833333,0\\n-0.0588235,0.547739,0.278689,-0.353535,0,-0.0342771,-0.688301,-0.2,0\\n-0.882353,0.286432,0.442623,-0.212121,-0.739953,0.0879285,-0.163962,-0.466667,0\\n-0.176471,0.376884,0.47541,-0.171717,0,-0.0461997,-0.732707,-0.4,1\\n0,0.236181,0.180328,0,0,0.0819672,-0.846285,0.0333333,0\\n-0.882353,0.0653266,0.245902,0,0,0.117735,-0.898377,-0.833333,1\\n-0.294118,0.909548,0.508197,0,0,0.0581222,-0.829206,0.5,0\\n-0.764706,-0.115578,-0.0491803,-0.474747,-0.962175,-0.153502,-0.412468,-0.966667,1\\n0.0588235,0.708543,0.213115,-0.373737,0,0.311475,-0.722459,-0.266667,0\\n0.0588235,-0.105528,0.0163934,0,0,-0.329359,-0.945346,-0.6,1\\n0.176471,0.0150754,0.245902,-0.030303,-0.574468,-0.019374,-0.920581,0.4,1\\n-0.764706,0.226131,0.147541,-0.454545,0,0.0968703,-0.77626,-0.8,1\\n-0.411765,0.21608,0.180328,-0.535354,-0.735225,-0.219076,-0.857387,-0.7,1\\n-0.882353,0.266332,-0.0163934,0,0,-0.102832,-0.768574,-0.133333,0\\n-0.882353,-0.0653266,0.147541,-0.373737,0,-0.0938897,-0.797609,-0.933333,1\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "LWbEvQzXZC5D"
      },
      "outputs": [],
      "source": [
        "xy = np.loadtxt('/content/data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
        "x_data = xy[:, 0:-1] # 행은 다가져오고 0~8까지 slicing\n",
        "y_data = xy[:, [-1]] # 행은 다가져오고 9만 slicing\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XurEjU_KZC5F",
        "outputId": "3e4a0a74-0cf2-41bb-ddd1-337da13f1c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n",
            "        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n",
            "        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n",
            "        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n",
            "        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000]])\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "print(x_train[0:5])\n",
        "print(y_train[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZuuOUtOZC5H"
      },
      "source": [
        "## Training with Real Data using low-level Binary Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU1GeTxCZC5H",
        "outputId": "0e483435-6b08-4f0e-c94f-0a262faf196d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/10000 Cost: 0.693147\n",
            "Epoch  100/10000 Cost: 0.480661\n",
            "Epoch  200/10000 Cost: 0.473846\n",
            "Epoch  300/10000 Cost: 0.472511\n",
            "Epoch  400/10000 Cost: 0.472051\n",
            "Epoch  500/10000 Cost: 0.471851\n",
            "Epoch  600/10000 Cost: 0.471758\n",
            "Epoch  700/10000 Cost: 0.471713\n",
            "Epoch  800/10000 Cost: 0.471692\n",
            "Epoch  900/10000 Cost: 0.471681\n",
            "Epoch 1000/10000 Cost: 0.471676\n",
            "Epoch 1100/10000 Cost: 0.471673\n",
            "Epoch 1200/10000 Cost: 0.471672\n",
            "Epoch 1300/10000 Cost: 0.471671\n",
            "Epoch 1400/10000 Cost: 0.471671\n",
            "Epoch 1500/10000 Cost: 0.471671\n",
            "Epoch 1600/10000 Cost: 0.471671\n",
            "Epoch 1700/10000 Cost: 0.471671\n",
            "Epoch 1800/10000 Cost: 0.471671\n",
            "Epoch 1900/10000 Cost: 0.471671\n",
            "Epoch 2000/10000 Cost: 0.471671\n",
            "Epoch 2100/10000 Cost: 0.471671\n",
            "Epoch 2200/10000 Cost: 0.471671\n",
            "Epoch 2300/10000 Cost: 0.471671\n",
            "Epoch 2400/10000 Cost: 0.471671\n",
            "Epoch 2500/10000 Cost: 0.471671\n",
            "Epoch 2600/10000 Cost: 0.471671\n",
            "Epoch 2700/10000 Cost: 0.471671\n",
            "Epoch 2800/10000 Cost: 0.471671\n",
            "Epoch 2900/10000 Cost: 0.471671\n",
            "Epoch 3000/10000 Cost: 0.471671\n",
            "Epoch 3100/10000 Cost: 0.471671\n",
            "Epoch 3200/10000 Cost: 0.471671\n",
            "Epoch 3300/10000 Cost: 0.471671\n",
            "Epoch 3400/10000 Cost: 0.471671\n",
            "Epoch 3500/10000 Cost: 0.471671\n",
            "Epoch 3600/10000 Cost: 0.471671\n",
            "Epoch 3700/10000 Cost: 0.471671\n",
            "Epoch 3800/10000 Cost: 0.471671\n",
            "Epoch 3900/10000 Cost: 0.471671\n",
            "Epoch 4000/10000 Cost: 0.471671\n",
            "Epoch 4100/10000 Cost: 0.471671\n",
            "Epoch 4200/10000 Cost: 0.471671\n",
            "Epoch 4300/10000 Cost: 0.471671\n",
            "Epoch 4400/10000 Cost: 0.471671\n",
            "Epoch 4500/10000 Cost: 0.471671\n",
            "Epoch 4600/10000 Cost: 0.471671\n",
            "Epoch 4700/10000 Cost: 0.471671\n",
            "Epoch 4800/10000 Cost: 0.471671\n",
            "Epoch 4900/10000 Cost: 0.471671\n",
            "Epoch 5000/10000 Cost: 0.471671\n",
            "Epoch 5100/10000 Cost: 0.471671\n",
            "Epoch 5200/10000 Cost: 0.471671\n",
            "Epoch 5300/10000 Cost: 0.471671\n",
            "Epoch 5400/10000 Cost: 0.471671\n",
            "Epoch 5500/10000 Cost: 0.471671\n",
            "Epoch 5600/10000 Cost: 0.471671\n",
            "Epoch 5700/10000 Cost: 0.471671\n",
            "Epoch 5800/10000 Cost: 0.471671\n",
            "Epoch 5900/10000 Cost: 0.471671\n",
            "Epoch 6000/10000 Cost: 0.471671\n",
            "Epoch 6100/10000 Cost: 0.471671\n",
            "Epoch 6200/10000 Cost: 0.471671\n",
            "Epoch 6300/10000 Cost: 0.471671\n",
            "Epoch 6400/10000 Cost: 0.471671\n",
            "Epoch 6500/10000 Cost: 0.471671\n",
            "Epoch 6600/10000 Cost: 0.471671\n",
            "Epoch 6700/10000 Cost: 0.471671\n",
            "Epoch 6800/10000 Cost: 0.471671\n",
            "Epoch 6900/10000 Cost: 0.471671\n",
            "Epoch 7000/10000 Cost: 0.471671\n",
            "Epoch 7100/10000 Cost: 0.471671\n",
            "Epoch 7200/10000 Cost: 0.471671\n",
            "Epoch 7300/10000 Cost: 0.471671\n",
            "Epoch 7400/10000 Cost: 0.471671\n",
            "Epoch 7500/10000 Cost: 0.471671\n",
            "Epoch 7600/10000 Cost: 0.471671\n",
            "Epoch 7700/10000 Cost: 0.471671\n",
            "Epoch 7800/10000 Cost: 0.471671\n",
            "Epoch 7900/10000 Cost: 0.471671\n",
            "Epoch 8000/10000 Cost: 0.471671\n",
            "Epoch 8100/10000 Cost: 0.471671\n",
            "Epoch 8200/10000 Cost: 0.471671\n",
            "Epoch 8300/10000 Cost: 0.471671\n",
            "Epoch 8400/10000 Cost: 0.471671\n",
            "Epoch 8500/10000 Cost: 0.471671\n",
            "Epoch 8600/10000 Cost: 0.471671\n",
            "Epoch 8700/10000 Cost: 0.471671\n",
            "Epoch 8800/10000 Cost: 0.471671\n",
            "Epoch 8900/10000 Cost: 0.471671\n",
            "Epoch 9000/10000 Cost: 0.471671\n",
            "Epoch 9100/10000 Cost: 0.471671\n",
            "Epoch 9200/10000 Cost: 0.471671\n",
            "Epoch 9300/10000 Cost: 0.471671\n",
            "Epoch 9400/10000 Cost: 0.471671\n",
            "Epoch 9500/10000 Cost: 0.471671\n",
            "Epoch 9600/10000 Cost: 0.471671\n",
            "Epoch 9700/10000 Cost: 0.471671\n",
            "Epoch 9800/10000 Cost: 0.471671\n",
            "Epoch 9900/10000 Cost: 0.471671\n",
            "Epoch 10000/10000 Cost: 0.471671\n"
          ]
        }
      ],
      "source": [
        "# 모델 초기화\n",
        "W = torch.zeros((8, 1), requires_grad=True) # 8차원 입력\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=1)\n",
        "\n",
        "nb_epochs = 10000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # Cost 계산\n",
        "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
        "    cost = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 10번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ6F-hfiZC5J"
      },
      "source": [
        "## Training with Real Data using `F.binary_cross_entropy`\n",
        "- 실데이터에 비해 모델이 단순하기 때문에 학습결과가 좋지 않다(cost가 줄지 않는다)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "qinHqfBDZC5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5894342-afd4-4de8-fa71-654ca1948204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1000 Cost: 0.693147\n",
            "Epoch   10/1000 Cost: 0.572727\n",
            "Epoch   20/1000 Cost: 0.539493\n",
            "Epoch   30/1000 Cost: 0.519708\n",
            "Epoch   40/1000 Cost: 0.507066\n",
            "Epoch   50/1000 Cost: 0.498539\n",
            "Epoch   60/1000 Cost: 0.492549\n",
            "Epoch   70/1000 Cost: 0.488209\n",
            "Epoch   80/1000 Cost: 0.484985\n",
            "Epoch   90/1000 Cost: 0.482543\n",
            "Epoch  100/1000 Cost: 0.480661\n",
            "Epoch  110/1000 Cost: 0.479189\n",
            "Epoch  120/1000 Cost: 0.478023\n",
            "Epoch  130/1000 Cost: 0.477088\n",
            "Epoch  140/1000 Cost: 0.476331\n",
            "Epoch  150/1000 Cost: 0.475711\n",
            "Epoch  160/1000 Cost: 0.475198\n",
            "Epoch  170/1000 Cost: 0.474771\n",
            "Epoch  180/1000 Cost: 0.474411\n",
            "Epoch  190/1000 Cost: 0.474107\n",
            "Epoch  200/1000 Cost: 0.473846\n",
            "Epoch  210/1000 Cost: 0.473622\n",
            "Epoch  220/1000 Cost: 0.473428\n",
            "Epoch  230/1000 Cost: 0.473259\n",
            "Epoch  240/1000 Cost: 0.473111\n",
            "Epoch  250/1000 Cost: 0.472980\n",
            "Epoch  260/1000 Cost: 0.472864\n",
            "Epoch  270/1000 Cost: 0.472761\n",
            "Epoch  280/1000 Cost: 0.472669\n",
            "Epoch  290/1000 Cost: 0.472586\n",
            "Epoch  300/1000 Cost: 0.472511\n",
            "Epoch  310/1000 Cost: 0.472444\n",
            "Epoch  320/1000 Cost: 0.472383\n",
            "Epoch  330/1000 Cost: 0.472327\n",
            "Epoch  340/1000 Cost: 0.472277\n",
            "Epoch  350/1000 Cost: 0.472230\n",
            "Epoch  360/1000 Cost: 0.472188\n",
            "Epoch  370/1000 Cost: 0.472149\n",
            "Epoch  380/1000 Cost: 0.472114\n",
            "Epoch  390/1000 Cost: 0.472081\n",
            "Epoch  400/1000 Cost: 0.472051\n",
            "Epoch  410/1000 Cost: 0.472023\n",
            "Epoch  420/1000 Cost: 0.471997\n",
            "Epoch  430/1000 Cost: 0.471974\n",
            "Epoch  440/1000 Cost: 0.471952\n",
            "Epoch  450/1000 Cost: 0.471932\n",
            "Epoch  460/1000 Cost: 0.471913\n",
            "Epoch  470/1000 Cost: 0.471896\n",
            "Epoch  480/1000 Cost: 0.471880\n",
            "Epoch  490/1000 Cost: 0.471865\n",
            "Epoch  500/1000 Cost: 0.471851\n",
            "Epoch  510/1000 Cost: 0.471839\n",
            "Epoch  520/1000 Cost: 0.471827\n",
            "Epoch  530/1000 Cost: 0.471816\n",
            "Epoch  540/1000 Cost: 0.471806\n",
            "Epoch  550/1000 Cost: 0.471796\n",
            "Epoch  560/1000 Cost: 0.471787\n",
            "Epoch  570/1000 Cost: 0.471779\n",
            "Epoch  580/1000 Cost: 0.471772\n",
            "Epoch  590/1000 Cost: 0.471765\n",
            "Epoch  600/1000 Cost: 0.471758\n",
            "Epoch  610/1000 Cost: 0.471752\n",
            "Epoch  620/1000 Cost: 0.471746\n",
            "Epoch  630/1000 Cost: 0.471741\n",
            "Epoch  640/1000 Cost: 0.471736\n",
            "Epoch  650/1000 Cost: 0.471732\n",
            "Epoch  660/1000 Cost: 0.471728\n",
            "Epoch  670/1000 Cost: 0.471724\n",
            "Epoch  680/1000 Cost: 0.471720\n",
            "Epoch  690/1000 Cost: 0.471717\n",
            "Epoch  700/1000 Cost: 0.471713\n",
            "Epoch  710/1000 Cost: 0.471710\n",
            "Epoch  720/1000 Cost: 0.471708\n",
            "Epoch  730/1000 Cost: 0.471705\n",
            "Epoch  740/1000 Cost: 0.471703\n",
            "Epoch  750/1000 Cost: 0.471701\n",
            "Epoch  760/1000 Cost: 0.471698\n",
            "Epoch  770/1000 Cost: 0.471697\n",
            "Epoch  780/1000 Cost: 0.471695\n",
            "Epoch  790/1000 Cost: 0.471693\n",
            "Epoch  800/1000 Cost: 0.471692\n",
            "Epoch  810/1000 Cost: 0.471690\n",
            "Epoch  820/1000 Cost: 0.471689\n",
            "Epoch  830/1000 Cost: 0.471688\n",
            "Epoch  840/1000 Cost: 0.471686\n",
            "Epoch  850/1000 Cost: 0.471685\n",
            "Epoch  860/1000 Cost: 0.471684\n",
            "Epoch  870/1000 Cost: 0.471683\n",
            "Epoch  880/1000 Cost: 0.471683\n",
            "Epoch  890/1000 Cost: 0.471682\n",
            "Epoch  900/1000 Cost: 0.471681\n",
            "Epoch  910/1000 Cost: 0.471680\n",
            "Epoch  920/1000 Cost: 0.471680\n",
            "Epoch  930/1000 Cost: 0.471679\n",
            "Epoch  940/1000 Cost: 0.471678\n",
            "Epoch  950/1000 Cost: 0.471678\n",
            "Epoch  960/1000 Cost: 0.471677\n",
            "Epoch  970/1000 Cost: 0.471677\n",
            "Epoch  980/1000 Cost: 0.471677\n",
            "Epoch  990/1000 Cost: 0.471676\n",
            "Epoch 1000/1000 Cost: 0.471676\n"
          ]
        }
      ],
      "source": [
        "# 모델 초기화\n",
        "W = torch.zeros((8, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # Cost 계산\n",
        "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
        "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 10번마다 로그 출력\n",
        "    if epoch % 10 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg3dl1RGZC5M"
      },
      "source": [
        "## Checking the Accuracy our our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VAVraCvZC5M"
      },
      "source": [
        "After we finish training the model, we want to check how well our model fits the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "HjCBGjfnZC5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3555f40f-215d-4cd3-bc25-542b13c5e5fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3555],\n",
            "        [0.9564],\n",
            "        [0.1934],\n",
            "        [0.9623],\n",
            "        [0.0706]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
        "print(hypothesis[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk1TYQfQZC5O"
      },
      "source": [
        "We can change **hypothesis** (real number from 0 to 1) to **binary predictions** (either 0 or 1) by comparing them to 0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "DsQyRkHbZC5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e99baa12-2002-450c-9a97-e9d167ff64d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[False],\n",
            "        [ True],\n",
            "        [False],\n",
            "        [ True],\n",
            "        [False]])\n"
          ]
        }
      ],
      "source": [
        "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
        "print(prediction[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oveMNvDcZC5R"
      },
      "source": [
        "Then, we compare it with the correct labels `y_train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "o7Dx1OqeZC5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfde5603-da2b-4453-9e81-79c43681ba6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[False],\n",
            "        [ True],\n",
            "        [False],\n",
            "        [ True],\n",
            "        [False]])\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "print(prediction[:5])\n",
        "print(y_train[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "wsuhFXmnZC5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37c84d5-76c6-44d0-961a-7ed467d6a084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True]])\n"
          ]
        }
      ],
      "source": [
        "correct_prediction = prediction.float() == y_train\n",
        "print(correct_prediction[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S5Me0vbZC5V"
      },
      "source": [
        "Finally, we can calculate the accuracy by counting the number of correct predictions and dividng by total number of predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "zNqEkPKzZC5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9c8f57-0993-44ed-8bcb-fb1566dabf6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has an accuracy of 76.94% for the training set.\n"
          ]
        }
      ],
      "source": [
        "accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
        "print('The model has an accuracy of {:2.2f}% for the training set.'.format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvhBuH4aZC5Y"
      },
      "source": [
        "## Optional: High-level Implementation with `nn.Module`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "aF6Aom_sZC5Y"
      },
      "outputs": [],
      "source": [
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(8, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.linear(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "yZsTEjbEZC5a"
      },
      "outputs": [],
      "source": [
        "model = BinaryClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "DIRf7VzaZC5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00b61c6-77e5-466a-e064-4ded5f7b260d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1000 Cost: 0.691402 Accuracy 55.47%\n",
            "Epoch   10/1000 Cost: 0.576110 Accuracy 66.93%\n",
            "Epoch   20/1000 Cost: 0.541669 Accuracy 72.33%\n",
            "Epoch   30/1000 Cost: 0.521164 Accuracy 76.28%\n",
            "Epoch   40/1000 Cost: 0.508066 Accuracy 76.55%\n",
            "Epoch   50/1000 Cost: 0.499238 Accuracy 77.08%\n",
            "Epoch   60/1000 Cost: 0.493040 Accuracy 77.08%\n",
            "Epoch   70/1000 Cost: 0.488553 Accuracy 77.08%\n",
            "Epoch   80/1000 Cost: 0.485224 Accuracy 76.42%\n",
            "Epoch   90/1000 Cost: 0.482704 Accuracy 76.42%\n",
            "Epoch  100/1000 Cost: 0.480765 Accuracy 76.81%\n",
            "Epoch  110/1000 Cost: 0.479250 Accuracy 76.94%\n",
            "Epoch  120/1000 Cost: 0.478053 Accuracy 76.94%\n",
            "Epoch  130/1000 Cost: 0.477094 Accuracy 76.94%\n",
            "Epoch  140/1000 Cost: 0.476319 Accuracy 76.94%\n",
            "Epoch  150/1000 Cost: 0.475686 Accuracy 76.94%\n",
            "Epoch  160/1000 Cost: 0.475164 Accuracy 76.94%\n",
            "Epoch  170/1000 Cost: 0.474730 Accuracy 77.08%\n",
            "Epoch  180/1000 Cost: 0.474366 Accuracy 77.08%\n",
            "Epoch  190/1000 Cost: 0.474058 Accuracy 77.08%\n",
            "Epoch  200/1000 Cost: 0.473796 Accuracy 77.08%\n",
            "Epoch  210/1000 Cost: 0.473572 Accuracy 77.08%\n",
            "Epoch  220/1000 Cost: 0.473377 Accuracy 77.21%\n",
            "Epoch  230/1000 Cost: 0.473209 Accuracy 77.21%\n",
            "Epoch  240/1000 Cost: 0.473062 Accuracy 77.21%\n",
            "Epoch  250/1000 Cost: 0.472932 Accuracy 77.08%\n",
            "Epoch  260/1000 Cost: 0.472818 Accuracy 77.08%\n",
            "Epoch  270/1000 Cost: 0.472716 Accuracy 77.08%\n",
            "Epoch  280/1000 Cost: 0.472626 Accuracy 77.08%\n",
            "Epoch  290/1000 Cost: 0.472545 Accuracy 77.08%\n",
            "Epoch  300/1000 Cost: 0.472472 Accuracy 77.08%\n",
            "Epoch  310/1000 Cost: 0.472407 Accuracy 77.21%\n",
            "Epoch  320/1000 Cost: 0.472347 Accuracy 77.21%\n",
            "Epoch  330/1000 Cost: 0.472294 Accuracy 77.21%\n",
            "Epoch  340/1000 Cost: 0.472245 Accuracy 77.08%\n",
            "Epoch  350/1000 Cost: 0.472200 Accuracy 77.08%\n",
            "Epoch  360/1000 Cost: 0.472160 Accuracy 77.08%\n",
            "Epoch  370/1000 Cost: 0.472122 Accuracy 77.08%\n",
            "Epoch  380/1000 Cost: 0.472088 Accuracy 77.08%\n",
            "Epoch  390/1000 Cost: 0.472057 Accuracy 77.08%\n",
            "Epoch  400/1000 Cost: 0.472029 Accuracy 77.08%\n",
            "Epoch  410/1000 Cost: 0.472002 Accuracy 77.08%\n",
            "Epoch  420/1000 Cost: 0.471978 Accuracy 76.94%\n",
            "Epoch  430/1000 Cost: 0.471955 Accuracy 76.94%\n",
            "Epoch  440/1000 Cost: 0.471935 Accuracy 76.94%\n",
            "Epoch  450/1000 Cost: 0.471916 Accuracy 76.94%\n",
            "Epoch  460/1000 Cost: 0.471898 Accuracy 76.94%\n",
            "Epoch  470/1000 Cost: 0.471882 Accuracy 76.94%\n",
            "Epoch  480/1000 Cost: 0.471867 Accuracy 76.94%\n",
            "Epoch  490/1000 Cost: 0.471853 Accuracy 76.94%\n",
            "Epoch  500/1000 Cost: 0.471840 Accuracy 76.94%\n",
            "Epoch  510/1000 Cost: 0.471828 Accuracy 76.94%\n",
            "Epoch  520/1000 Cost: 0.471817 Accuracy 76.94%\n",
            "Epoch  530/1000 Cost: 0.471806 Accuracy 76.81%\n",
            "Epoch  540/1000 Cost: 0.471797 Accuracy 76.81%\n",
            "Epoch  550/1000 Cost: 0.471788 Accuracy 76.81%\n",
            "Epoch  560/1000 Cost: 0.471780 Accuracy 76.81%\n",
            "Epoch  570/1000 Cost: 0.471772 Accuracy 76.68%\n",
            "Epoch  580/1000 Cost: 0.471765 Accuracy 76.68%\n",
            "Epoch  590/1000 Cost: 0.471758 Accuracy 76.81%\n",
            "Epoch  600/1000 Cost: 0.471752 Accuracy 76.81%\n",
            "Epoch  610/1000 Cost: 0.471747 Accuracy 76.81%\n",
            "Epoch  620/1000 Cost: 0.471741 Accuracy 76.81%\n",
            "Epoch  630/1000 Cost: 0.471736 Accuracy 76.81%\n",
            "Epoch  640/1000 Cost: 0.471732 Accuracy 76.81%\n",
            "Epoch  650/1000 Cost: 0.471728 Accuracy 76.81%\n",
            "Epoch  660/1000 Cost: 0.471724 Accuracy 76.81%\n",
            "Epoch  670/1000 Cost: 0.471720 Accuracy 76.81%\n",
            "Epoch  680/1000 Cost: 0.471717 Accuracy 76.81%\n",
            "Epoch  690/1000 Cost: 0.471713 Accuracy 76.81%\n",
            "Epoch  700/1000 Cost: 0.471710 Accuracy 76.81%\n",
            "Epoch  710/1000 Cost: 0.471708 Accuracy 76.81%\n",
            "Epoch  720/1000 Cost: 0.471705 Accuracy 76.94%\n",
            "Epoch  730/1000 Cost: 0.471703 Accuracy 76.94%\n",
            "Epoch  740/1000 Cost: 0.471701 Accuracy 76.94%\n",
            "Epoch  750/1000 Cost: 0.471698 Accuracy 76.94%\n",
            "Epoch  760/1000 Cost: 0.471697 Accuracy 76.94%\n",
            "Epoch  770/1000 Cost: 0.471695 Accuracy 76.94%\n",
            "Epoch  780/1000 Cost: 0.471693 Accuracy 76.94%\n",
            "Epoch  790/1000 Cost: 0.471692 Accuracy 76.94%\n",
            "Epoch  800/1000 Cost: 0.471690 Accuracy 76.94%\n",
            "Epoch  810/1000 Cost: 0.471689 Accuracy 76.94%\n",
            "Epoch  820/1000 Cost: 0.471688 Accuracy 76.94%\n",
            "Epoch  830/1000 Cost: 0.471686 Accuracy 76.94%\n",
            "Epoch  840/1000 Cost: 0.471685 Accuracy 76.94%\n",
            "Epoch  850/1000 Cost: 0.471684 Accuracy 76.94%\n",
            "Epoch  860/1000 Cost: 0.471683 Accuracy 76.94%\n",
            "Epoch  870/1000 Cost: 0.471683 Accuracy 76.94%\n",
            "Epoch  880/1000 Cost: 0.471682 Accuracy 76.94%\n",
            "Epoch  890/1000 Cost: 0.471681 Accuracy 76.94%\n",
            "Epoch  900/1000 Cost: 0.471680 Accuracy 76.94%\n",
            "Epoch  910/1000 Cost: 0.471680 Accuracy 76.94%\n",
            "Epoch  920/1000 Cost: 0.471679 Accuracy 76.94%\n",
            "Epoch  930/1000 Cost: 0.471678 Accuracy 76.94%\n",
            "Epoch  940/1000 Cost: 0.471678 Accuracy 76.94%\n",
            "Epoch  950/1000 Cost: 0.471677 Accuracy 76.94%\n",
            "Epoch  960/1000 Cost: 0.471677 Accuracy 76.94%\n",
            "Epoch  970/1000 Cost: 0.471676 Accuracy 76.94%\n",
            "Epoch  980/1000 Cost: 0.471676 Accuracy 76.94%\n",
            "Epoch  990/1000 Cost: 0.471676 Accuracy 76.94%\n",
            "Epoch 1000/1000 Cost: 0.471675 Accuracy 76.94%\n"
          ]
        }
      ],
      "source": [
        "# optimizer 설정\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # 20번마다 로그 출력\n",
        "    if epoch % 10 == 0:\n",
        "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
        "        correct_prediction = prediction.float() == y_train\n",
        "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(\n",
        "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
        "        ))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9ZuuOUtOZC5H",
        "fQ6F-hfiZC5J",
        "dg3dl1RGZC5M",
        "lvhBuH4aZC5Y"
      ],
      "name": "Day 2_02.logistic_classification.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}